from typing import TypedDict, Annotated, List, Dict, Any
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.document_compressors import (
    CrossEncoderReranker,
    LLMChainExtractor,
    DocumentCompressorPipeline,
    EmbeddingsFilter
)
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.document_transformers import EmbeddingsRedundantFilter
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser, BaseOutputParser
from textwrap import dedent
import operator
import os
from pathlib import Path
from dotenv import load_dotenv
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False
    print("âš ï¸ Kiwi í† í¬ë‚˜ì´ì €ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    query: str
    search_results: List[Document]
    context: str
    final_answer: str
    search_strategy: str  # "general", "specific", "troubleshooting"
    search_method: str   # "hybrid_semantic", "hybrid_balanced", "hybrid_keyword"
    confidence_score: float
    page_references: List[int]
    need_clarification: bool

# ì „ì—­ ë³€ìˆ˜ë¡œ ê²€ìƒ‰ê¸°ë“¤ ê´€ë¦¬
vector_store = None
bm25_retriever = None
hybrid_retriever = None
kiwi_model = None
multi_query_retriever = None
cross_encoder_retriever = None
compression_retriever = None

# ë„êµ¬ë“¤ ì •ì˜
@tool
def vector_search(query: str, top_k: int = 5) -> List[Dict]:
    """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°"""
    global vector_store
    if vector_store is None:
        return [{"content": "ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0, "score": 0.0}]
    
    try:
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        results = vector_store.similarity_search_with_score(query, k=top_k)
        
        search_results = []
        for doc, score in results:
            search_results.append({
                "content": doc.page_content,
                "page": doc.metadata.get("page", 0),
                "source": doc.metadata.get("source", ""),
                "score": float(score)
            })
        
        return search_results
    except Exception as e:
        return [{"content": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}]

@tool
def bm25_search(query: str, top_k: int = 5) -> List[Dict]:
    """í‚¤ì›Œë“œ ê¸°ë°˜ BM25 ê²€ìƒ‰"""
    global bm25_retriever
    if bm25_retriever is None:
        return [{"content": "BM25 ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0, "score": 0.0}]
    
    try:
        # BM25 ê²€ìƒ‰ ìˆ˜í–‰
        results = bm25_retriever.invoke(query)
        
        search_results = []
        for doc in results[:top_k]:
            search_results.append({
                "content": doc.page_content,
                "page": doc.metadata.get("page", 0),
                "source": doc.metadata.get("source", ""),
                "score": 1.0  # BM25ëŠ” ì ìˆ˜ë¥¼ ì§ì ‘ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
            })
        
        return search_results
    except Exception as e:
        return [{"content": f"BM25 ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}]

@tool
def hybrid_search(query: str, top_k: int = 5, semantic_weight: float = 0.5) -> List[Dict]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + BM25)"""
    global hybrid_retriever, vector_store, bm25_retriever
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°ê°€ ì—†ê±°ë‚˜ ê°€ì¤‘ì¹˜ê°€ ë‹¤ë¥¸ ê²½ìš° ì¬ìƒì„±
    if (hybrid_retriever is None or 
        getattr(hybrid_retriever, '_weights', [0.5, 0.5])[0] != semantic_weight):
        
        if vector_store is None or bm25_retriever is None:
            return [{"content": "ë²¡í„° ì €ì¥ì†Œ ë˜ëŠ” BM25 ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0, "score": 0.0}]
        
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ìƒì„±
            semantic_retriever = vector_store.as_retriever(search_kwargs={"k": top_k})
            keyword_weight = 1.0 - semantic_weight
            
            hybrid_retriever = EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[semantic_weight, keyword_weight]
            )
            hybrid_retriever._weights = [semantic_weight, keyword_weight]  # ê°€ì¤‘ì¹˜ ì €ì¥
            
        except Exception as e:
            return [{"content": f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ìƒì„± ì˜¤ë¥˜: {str(e)}", "page": 0, "score": 0.0}]
    
    try:
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        results = hybrid_retriever.invoke(query)
        
        search_results = []
        for doc in results[:top_k]:
            search_results.append({
                "content": doc.page_content,
                "page": doc.metadata.get("page", 0),
                "source": doc.metadata.get("source", ""),
                "score": 1.0  # í•˜ì´ë¸Œë¦¬ë“œëŠ” ì ìˆ˜ë¥¼ ì§ì ‘ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
            })
        
        return search_results
    except Exception as e:
        return [{"content": f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}]

@tool
def multi_query_search(query: str, top_k: int = 5) -> List[Dict]:
    """ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± í›„ ê²€ìƒ‰"""
    global multi_query_retriever
    
    if multi_query_retriever is None:
        return [{"content": "ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0, "score": 0.0}]
    
    try:
        # ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ ìˆ˜í–‰
        results = multi_query_retriever.invoke(query)
        
        search_results = []
        for doc in results[:top_k]:
            search_results.append({
                "content": doc.page_content,
                "page": doc.metadata.get("page", 0),
                "source": doc.metadata.get("source", ""),
                "score": 1.0
            })
        
        return search_results
    except Exception as e:
        return [{"content": f"ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}]

@tool
def expanded_query_search(query: str, top_k: int = 5) -> List[Dict]:
    """ì°¨ëŸ‰ ì „ë¬¸ ìš©ì–´ë¡œ ì¿¼ë¦¬ í™•ì¥ í›„ ê²€ìƒ‰"""
    global hybrid_retriever
    
    if hybrid_retriever is None:
        return [{"content": "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0, "score": 0.0}]
    
    try:
        # ì°¨ëŸ‰ ì „ë¬¸ ìš©ì–´ ë§¤í•‘
        vehicle_terms = {
            'íƒ€ì´ì–´': ['íƒ€ì´ì–´', 'íƒ€ì´ì–´ ì••ë ¥', 'ê³µê¸°ì••', 'ì••ë ¥', 'PSI', 'bar'],
            'ë¸Œë ˆì´í¬': ['ë¸Œë ˆì´í¬', 'ì œë™', 'ì œë™ì¥ì¹˜', 'ë¸Œë ˆì´í¬ íŒ¨ë“œ', 'ì œë™ íŒ¨ë“œ'],
            'ì—”ì§„': ['ì—”ì§„', 'ëª¨í„°', 'ë™ë ¥ì¥ì¹˜', 'ì—”ì§„ì˜¤ì¼', 'ì˜¤ì¼'],
            'ê²½ê³ ë“±': ['ê²½ê³ ë“±', 'í‘œì‹œë“±', 'ì¸ë””ì¼€ì´í„°', 'ì•Œë¦¼ë“±', 'ê²½ê³  ì‹ í˜¸'],
            'ë¬¸ì œ': ['ë¬¸ì œ', 'ê³ ì¥', 'ì˜¤ë¥˜', 'ì´ìƒ', 'ê²°í•¨', 'ì˜¤ì‘ë™'],
            'êµì²´': ['êµì²´', 'êµì²´ë°©ë²•', 'ë°”ê¿€ê¸°', 'ìˆ˜ë¦¬', 'ì •ë¹„', 'ìœ ì§€ë³´ìˆ˜']
        }
        
        # ì¿¼ë¦¬ í™•ì¥
        expanded_terms = []
        query_lower = query.lower()
        
        for key, synonyms in vehicle_terms.items():
            if key in query_lower:
                expanded_terms.extend(synonyms)
        
        # ê¸°ë³¸ ì¿¼ë¦¬ì— í™•ì¥ ìš©ì–´ ì¶”ê°€
        if expanded_terms:
            expanded_query = f"{query} {' '.join(expanded_terms)}"
        else:
            expanded_query = query
        
        # í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        results = hybrid_retriever.invoke(expanded_query)
        
        search_results = []
        for doc in results[:top_k]:
            search_results.append({
                "content": doc.page_content,
                "page": doc.metadata.get("page", 0),
                "source": doc.metadata.get("source", ""),
                "score": 1.0
            })
        
        return search_results
    except Exception as e:
        return [{"content": f"í™•ì¥ ì¿¼ë¦¬ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}]

@tool
def cross_encoder_rerank_search(query: str, top_k: int = 5) -> List[Dict]:
    """Cross-Encoder ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¬ìˆœìœ„í™” ê²€ìƒ‰"""
    global cross_encoder_retriever
    
    if cross_encoder_retriever is None:
        return [{"content": "Cross-Encoder ì¬ìˆœìœ„í™” ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0, "score": 0.0}]
    
    try:
        # Cross-Encoder ì¬ìˆœìœ„í™” ê²€ìƒ‰ ìˆ˜í–‰
        results = cross_encoder_retriever.invoke(query)
        
        search_results = []
        for doc in results[:top_k]:
            search_results.append({
                "content": doc.page_content,
                "page": doc.metadata.get("page", 0),
                "source": doc.metadata.get("source", ""),
                "score": getattr(doc, 'score', 1.0)  # Cross-Encoder ì ìˆ˜
            })
        
        return search_results
    except Exception as e:
        return [{"content": f"Cross-Encoder ì¬ìˆœìœ„í™” ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}]

@tool
def contextual_compression_search(query: str, top_k: int = 5) -> List[Dict]:
    """ë§¥ë½ ì••ì¶•ì„ ì‚¬ìš©í•œ ê²€ìƒ‰ (ê´€ë ¨ ì •ë³´ë§Œ ì¶”ì¶œ)"""
    global compression_retriever
    
    if compression_retriever is None:
        return [{"content": "ë§¥ë½ ì••ì¶• ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0, "score": 0.0}]
    
    try:
        # ë§¥ë½ ì••ì¶• ê²€ìƒ‰ ìˆ˜í–‰
        results = compression_retriever.invoke(query)
        
        search_results = []
        for doc in results[:top_k]:
            search_results.append({
                "content": doc.page_content,
                "page": doc.metadata.get("page", 0),
                "source": doc.metadata.get("source", ""),
                "score": 1.0  # ì••ì¶•ëœ ë¬¸ì„œëŠ” ëª¨ë‘ ê´€ë ¨ì„± ë†’ìŒ
            })
        
        return search_results
    except Exception as e:
        return [{"content": f"ë§¥ë½ ì••ì¶• ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}]

@tool  
def keyword_search(query: str, keywords: List[str] = None) -> List[Dict]:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì •í™•í•œ ê²€ìƒ‰"""
    global vector_store
    if vector_store is None:
        return [{"content": "ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0}]
    
    try:
        # í‚¤ì›Œë“œê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ
        if keywords is None:
            keywords = query.split()
        
        # í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ í™•ì¥ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        expanded_query = f"{query} {' '.join(keywords)}"
        results = vector_store.similarity_search(expanded_query, k=3)
        
        search_results = []
        for doc in results:
            search_results.append({
                "content": doc.page_content,
                "page": doc.metadata.get("page", 0),
                "source": doc.metadata.get("source", "")
            })
        
        return search_results
    except Exception as e:
        return [{"content": f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0}]

@tool
def page_context_search(page_numbers: List[int]) -> List[Dict]:
    """íŠ¹ì • í˜ì´ì§€ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""
    global vector_store
    if vector_store is None:
        return [{"content": "ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "page": 0}]
    
    try:
        results = []
        for page_num in page_numbers:
            # íŠ¹ì • í˜ì´ì§€ì™€ ì¸ì ‘ í˜ì´ì§€ ê²€ìƒ‰
            for offset in [-1, 0, 1]:  # ì´ì „, í˜„ì¬, ë‹¤ìŒ í˜ì´ì§€
                target_page = page_num + offset
                if target_page > 0:
                    # í˜ì´ì§€ ë²ˆí˜¸ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰
                    page_docs = vector_store.get(where={"page": target_page})
                    if page_docs and 'documents' in page_docs:
                        for i, content in enumerate(page_docs['documents']):
                            metadata = page_docs.get('metadatas', [{}])[i] if page_docs.get('metadatas') else {}
                            results.append({
                                "content": content,
                                "page": target_page,
                                "source": metadata.get("source", "")
                            })
        
        return results if results else [{"content": "í•´ë‹¹ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "page": 0}]
    except Exception as e:
        return [{"content": f"í˜ì´ì§€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0}]

# ë…¸ë“œë“¤ ì •ì˜
class VehicleManualAgent:
    def __init__(self, pdf_path: str = None):
        self.llm = ChatOpenAI(temperature=0)
        self.embeddings = OpenAIEmbeddings()
        self.pdf_path = pdf_path or "/Users/1112931/llm-3/project/data/backup/kr_ko-KR_xc60_2026.pdf"
        self.vector_db = None
        self.tools = [vector_search, keyword_search, page_context_search, bm25_search, hybrid_search, multi_query_search, expanded_query_search, cross_encoder_rerank_search, contextual_compression_search]
        
        # Few-shot ì˜ˆì‹œ ë°ì´í„° ì´ˆê¸°í™”
        self._initialize_few_shot_examples()
        
        # ê²€ìƒ‰ ì „ëµ ì˜µì…˜
        self.search_options = {
            "vector_only": "ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©",
            "bm25_only": "BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš©", 
            "hybrid_balanced": "í•˜ì´ë¸Œë¦¬ë“œ (5:5)",
            "hybrid_semantic": "í•˜ì´ë¸Œë¦¬ë“œ (7:3 - ì˜ë¯¸ë¡ ì  ìš°ì„ )",
            "hybrid_keyword": "í•˜ì´ë¸Œë¦¬ë“œ (3:7 - í‚¤ì›Œë“œ ìš°ì„ )",
            "multi_query": "ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± ê²€ìƒ‰",
            "expanded_query": "ì „ë¬¸ ìš©ì–´ í™•ì¥ ê²€ìƒ‰",
            "cross_encoder_rerank": "Cross-Encoder ì¬ìˆœìœ„í™” ê²€ìƒ‰",
            "contextual_compression": "ë§¥ë½ ì••ì¶• ê²€ìƒ‰"
        }
        
        # PDF ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        self._initialize_vector_store()
    
    def _initialize_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ë° PDF ë¬¸ì„œ ë¡œë“œ"""
        global vector_store
        
        try:
            # PDF íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(self.pdf_path):
                print(f"ê²½ê³ : PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.pdf_path}")
                return
            
            print(f"PDF íŒŒì¼ ë¡œë”© ì¤‘: {self.pdf_path}")
            
            # PDF ë¡œë”ë¡œ ë¬¸ì„œ ë¡œë“œ
            loader = PyPDFLoader(self.pdf_path)
            documents = loader.load()
            
            print(f"ë¡œë“œëœ í˜ì´ì§€ ìˆ˜: {len(documents)}")
            
            # í…ìŠ¤íŠ¸ ë¶„í•  (í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• )
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=300,  # ë”ë” ì‘ì€ ì²­í¬ í¬ê¸°
                chunk_overlap=30,  # ê²©ì²­ ê°„ê²© ì¶•ì†Œ
                length_function=len,
            )
            
            split_docs = text_splitter.split_documents(documents)
            print(f"ë¶„í• ëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(split_docs)}")
            
            # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            persist_directory = "/Users/1112931/llm-3/project/chroma_db"
            
            # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸
            if os.path.exists(persist_directory) and os.listdir(persist_directory):
                print("ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
                vector_store = Chroma(
                    persist_directory=persist_directory,
                    embedding_function=self.embeddings
                )
            else:
                print("ìƒˆë¡œìš´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
                
                # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¬¸ì„œ ì¶”ê°€ (í† í° ì œí•œ íšŒí”¼)
                batch_size = 50  # ë°°ì¹˜ í¬ê¸° ì„¤ì •
                vector_store = Chroma(
                    persist_directory=persist_directory,
                    embedding_function=self.embeddings
                )
                
                print(f"ì´ {len(split_docs)}ê°œ ë¬¸ì„œë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬...")
                
                for i in range(0, len(split_docs), batch_size):
                    batch = split_docs[i:i+batch_size]
                    try:
                        vector_store.add_documents(batch)
                        print(f"ë°°ì¹˜ {i//batch_size + 1}/{(len(split_docs)-1)//batch_size + 1} ì™„ë£Œ")
                    except Exception as batch_error:
                        print(f"ë°°ì¹˜ {i//batch_size + 1} ì˜¤ë¥˜: {str(batch_error)}")
                        continue
                
                # BM25 ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
                self._initialize_bm25_retriever(split_docs)
            
            self.vector_db = vector_store
            print("ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ!")
            
            # MultiQueryRetriever ì´ˆê¸°í™”
            self._initialize_multi_query_retriever()
            
            # Cross-Encoder ì¬ìˆœìœ„í™” ì´ˆê¸°í™”
            self._initialize_cross_encoder_retriever()
            
            # ë§¥ë½ ì••ì¶• ì´ˆê¸°í™”
            self._initialize_contextual_compression()
            
            # BM25 ê²€ìƒ‰ê¸° ì´ˆê¸°í™” (ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œì‹œ)
            if os.path.exists(persist_directory) and os.listdir(persist_directory):
                # ì €ì¥ëœ ë¬¸ì„œë“¤ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ BM25 ê²€ìƒ‰ê¸° ìƒì„±
                try:
                    all_docs = vector_store.get()
                    if all_docs and 'documents' in all_docs and all_docs['documents']:
                        # ë¬¸ì„œë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
                        documents_for_bm25 = []
                        for i, content in enumerate(all_docs['documents']):
                            metadata = all_docs.get('metadatas', [{}])[i] if all_docs.get('metadatas') else {}
                            documents_for_bm25.append(Document(page_content=content, metadata=metadata))
                        
                        self._initialize_bm25_retriever(documents_for_bm25)
                except Exception as e:
                    print(f"BM25 ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ê¸°ì¡´ DB): {str(e)}")
            
        except Exception as e:
            print(f"ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            vector_store = None
    
    def _setup_korean_tokenizer(self):
        """í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì„¤ì •"""
        global kiwi_model
        
        if not KIWI_AVAILABLE:
            return None
            
        try:
            kiwi_model = Kiwi()
            # ì‚¬ìš©ì ì •ì˜ ë‹¨ì–´ ì¶”ê°€
            custom_words = [
                ('ë³¼ë³´', 'NNP'),
                ('XC60', 'NNP'), 
                ('íƒ€ì´ì–´', 'NNG'),
                ('ë¸Œë ˆì´í¬', 'NNG'),
                ('ì—”ì§„', 'NNG'),
                ('ê²½ê³ ë“±', 'NNG')
            ]
            
            for word, pos in custom_words:
                kiwi_model.add_user_word(word, pos)
                
            print(f"âœ… í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì„¤ì • ì™„ë£Œ: {len(custom_words)}ê°œ ì‚¬ìš©ì ë‹¨ì–´ ì¶”ê°€")
            return kiwi_model
            
        except Exception as e:
            print(f"í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _korean_tokenizer(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•"""
        if kiwi_model is None:
            # Kiwiê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê³µë°± ë¶„ë¦¬
            return text.split()
        
        try:
            # Kiwië¡œ í† í¬ë‚˜ì´ì§•
            tokens = []
            for token, pos, _, _ in kiwi_model.analyze(text):
                # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ê³ ìœ ëª…ì‚¬ë§Œ ì¶”ì¶œ
                if pos.startswith(('NN', 'VV', 'VA', 'NP')):
                    tokens.append(token)
            return tokens if tokens else text.split()
        except:
            return text.split()
    
    def _initialize_bm25_retriever(self, documents: List[Document]):
        """BM25 ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        global bm25_retriever
        
        try:
            # í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì„¤ì •
            self._setup_korean_tokenizer()
            
            # BM25 ê²€ìƒ‰ê¸° ìƒì„±
            if KIWI_AVAILABLE and kiwi_model is not None:
                # í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                bm25_retriever = BM25Retriever.from_documents(
                    documents=documents,
                    preprocess_func=self._korean_tokenizer,
                    k=5
                )
                print("âœ… BM25 ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì ìš©)")
            else:
                # ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                bm25_retriever = BM25Retriever.from_documents(
                    documents=documents,
                    k=5
                )
                print("âœ… BM25 ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ (ê¸°ë³¸ í† í¬ë‚˜ì´ì €)")
                
        except Exception as e:
            print(f"BM25 ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            bm25_retriever = None
    
    def _initialize_multi_query_retriever(self):
        """MultiQueryRetriever ì´ˆê¸°í™”"""
        global multi_query_retriever, vector_store
        
        if vector_store is None:
            print("âš ï¸ ë²¡í„° ì €ì¥ì†Œê°€ ì—†ì–´ MultiQueryRetrieverë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ì°¨ëŸ‰ ë§¤ë‰´ì–¼ ì „ìš© ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
            vehicle_prompt = ChatPromptTemplate.from_template(
                """ì°¨ëŸ‰ ë§¤ë‰´ì–¼ ê²€ìƒ‰ì„ ìœ„í•´ ì£¼ì–´ì§„ ì§ˆë¬¸ì„ 3ê°œì˜ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.
                
                ì›ë³¸ ì§ˆë¬¸: {question}
                
                ë‹¤ìŒ ê´€ì ë“¤ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
                1. ê¸°ìˆ ì /ì „ë¬¸ì  ê´€ì 
                2. ì‚¬ìš©ì/ì‹¤ìš©ì  ê´€ì   
                3. ë¬¸ì œí•´ê²°/ì•ˆì „ ê´€ì 
                
                ê° ì§ˆë¬¸ì€ í•œ ì¤„ë¡œ ì‘ì„±í•˜ê³ , ìˆ«ìë‚˜ ë¶€í˜¸ ì—†ì´ ì‘ì„±í•´ì£¼ì„¸ìš”.
                """
            )
            
            # ì»¤ìŠ¤í…€ ì¶œë ¥ íŒŒì„œ
            class LineListOutputParser(BaseOutputParser[List[str]]):
                def parse(self, text: str) -> List[str]:
                    lines = text.strip().split('\n')
                    return [line.strip() for line in lines if line.strip()]
            
            # ë‹¤ì¤‘ ì¿¼ë¦¬ ì²´ì¸ ìƒì„±
            multi_query_chain = vehicle_prompt | self.llm | LineListOutputParser()
            
            # MultiQueryRetriever ìƒì„±
            base_retriever = vector_store.as_retriever(search_kwargs={"k": 3})
            multi_query_retriever = MultiQueryRetriever(
                retriever=base_retriever,
                llm_chain=multi_query_chain,
                parser_key="lines"
            )
            
            print("âœ… MultiQueryRetriever ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"MultiQueryRetriever ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            multi_query_retriever = None
    
    def _initialize_cross_encoder_retriever(self):
        """Cross-Encoder ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¬ìˆœìœ„í™” ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        global cross_encoder_retriever, vector_store
        
        if vector_store is None:
            print("âš ï¸ ë²¡í„° ì €ì¥ì†Œê°€ ì—†ì–´ Cross-Encoder ì¬ìˆœìœ„í™”ê¸°ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            print("ğŸ”„ Cross-Encoder ì¬ìˆœìœ„í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # Cross-Encoder ëª¨ë¸ ë¡œë“œ (ê²½ëŸ‰í™”ëœ ëª¨ë¸ ì‚¬ìš©)
            cross_encoder_model = HuggingFaceCrossEncoder(
                model_name="BAAI/bge-reranker-v2-m3"  # ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸
            )
            
            # ì¬ìˆœìœ„í™” ì»´í”„ë ˆì„œ ìƒì„±
            reranker = CrossEncoderReranker(
                model=cross_encoder_model,
                top_n=5  # ìƒìœ„ 5ê°œ ë¬¸ì„œë§Œ ë°˜í™˜
            )
            
            # ë² ì´ìŠ¤ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± (ë” ë§ì€ í›„ë³´ ë¬¸ì„œ ê²€ìƒ‰)
            base_retriever = vector_store.as_retriever(search_kwargs={"k": 20})  # 20ê°œ í›„ë³´ ê²€ìƒ‰
            
            # ì»¨í…ìŠ¤ì¸„ì–¼ ì»´í”„ë ˆì…˜ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
            cross_encoder_retriever = ContextualCompressionRetriever(
                base_compressor=reranker,
                base_retriever=base_retriever
            )
            
            print("âœ… Cross-Encoder ì¬ìˆœìœ„í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   ëª¨ë¸: BAAI/bge-reranker-v2-m3")
            print(f"   í›„ë³´ ë¬¸ì„œ: 20ê°œ -> ìƒìœ„ 5ê°œ ì„ ë³„")
            
        except Exception as e:
            print(f"Cross-Encoder ì¬ìˆœìœ„í™” ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            cross_encoder_retriever = None
    
    def _initialize_contextual_compression(self):
        """ë§¥ë½ ì••ì¶• ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        global compression_retriever, vector_store
        
        if vector_store is None:
            print("âš ï¸ ë²¡í„° ì €ì¥ì†Œê°€ ì—†ì–´ ë§¥ë½ ì••ì¶•ê¸°ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            print("ğŸ“ ë§¥ë½ ì••ì¶• ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # 1. ì„ë² ë”© ê¸°ë°˜ í•„í„°ë§ (ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •)
            embeddings_filter = EmbeddingsFilter(
                embeddings=self.embeddings,
                similarity_threshold=0.6  # 60% ì´ìƒ ìœ ì‚¬í•œ ë¬¸ì„œë§Œ í†µê³¼
            )
            
            # 2. ì¤‘ë³µ ì œê±° í•„í„°
            redundant_filter = EmbeddingsRedundantFilter(
                embeddings=self.embeddings,
                similarity_threshold=0.9  # 90% ì´ìƒ ìœ ì‚¬í•œ ë¬¸ì„œ ì œê±°
            )
            
            # 3. LLM ê¸°ë°˜ ì •ë³´ ì¶”ì¶œê¸°
            llm_extractor = LLMChainExtractor.from_llm(llm=self.llm)
            
            # ë‹¤ë‹¨ê³„ ì••ì¶• íŒŒì´í”„ë¼ì¸ êµ¬ì„±
            pipeline_compressor = DocumentCompressorPipeline(
                transformers=[
                    embeddings_filter,      # 1ë‹¨ê³„: ìœ ì‚¬ë„ í•„í„°ë§
                    redundant_filter,       # 2ë‹¨ê³„: ì¤‘ë³µ ì œê±°
                    llm_extractor          # 3ë‹¨ê³„: í•µì‹¬ ì •ë³´ ì¶”ì¶œ
                ]
            )
            
            # ë² ì´ìŠ¤ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
            base_retriever = vector_store.as_retriever(search_kwargs={"k": 15})  # 15ê°œ í›„ë³´ ê²€ìƒ‰
            
            # ì»¨í…ìŠ¤ì¸„ì–¼ ì»´í”„ë ˆì…˜ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
            compression_retriever = ContextualCompressionRetriever(
                base_compressor=pipeline_compressor,
                base_retriever=base_retriever
            )
            
            print("âœ… ë§¥ë½ ì••ì¶• ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   1ë‹¨ê³„: ì„ë² ë”© í•„í„°ë§ (ìœ ì‚¬ë„ > 60%)")
            print(f"   2ë‹¨ê³„: ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ > 90%)")
            print(f"   3ë‹¨ê³„: LLM ê¸°ë°˜ í•µì‹¬ ì •ë³´ ì¶”ì¶œ")
            
        except Exception as e:
            print(f"ë§¥ë½ ì••ì¶• ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            compression_retriever = None
    
    def _initialize_few_shot_examples(self):
        """ì°¨ëŸ‰ ë§¤ë‰´ì–¼ ê´€ë ¨ Few-shot ì˜ˆì‹œ ë°ì´í„° ì´ˆê¸°í™”"""
        
        # ì°¨ëŸ‰ ë§¤ë‰´ì–¼ ì§ˆë¬¸-ë‹µë³€ ì˜ˆì‹œ ë°ì´í„°
        self.vehicle_examples = [
            {
                "query": "ì˜¤ì¼ êµì²´ëŠ” ì–¸ì œ í•´ì•¼ í•˜ë‚˜ìš”?",
                "context": "ì—”ì§„ ì˜¤ì¼ì€ ì£¼í–‰ ê±°ë¦¬ì— ë”°ë¼ êµì²´ ì£¼ê¸°ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ 10,000km ë˜ëŠ” 12ê°œì›”ë§ˆë‹¤ êµì²´í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤. ì˜¤ì¼ êµì²´ëŠ” ë³¼ë³´ ê³µì‹ ì„œë¹„ìŠ¤ ì„¼í„°ì—ì„œ ì‹¤ì‹œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. [í˜ì´ì§€ 326-327]",
                "answer": "ì—”ì§„ ì˜¤ì¼ì€ ì£¼í–‰ ê±°ë¦¬ 10,000km ë˜ëŠ” 12ê°œì›”ë§ˆë‹¤ êµì²´í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤. ì •í™•í•œ êµì²´ ì‹œê¸°ëŠ” ì£¼í–‰ í™˜ê²½ê³¼ ìš´ì „ ìŠµê´€ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ, ë³¼ë³´ ê³µì‹ ì„œë¹„ìŠ¤ ì„¼í„°ì—ì„œ ì ê²€ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ì•ˆì „ìƒ ì£¼ì˜ì‚¬í•­ìœ¼ë¡œ ì˜¤ì¼ ë ˆë²¨ì„ ì •ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n\nğŸ“š ì°¸ê³  í˜ì´ì§€: 326-327"
            },
            {
                "query": "ë¸Œë ˆì´í¬ê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²ƒ ê°™ì•„ìš”.",
                "context": "ë¸Œë ˆì´í¬ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìƒê²¼ì„ ë•ŒëŠ” ì¦‰ì‹œ ì£¼í–‰ì„ ì¤‘ë‹¨í•˜ê³  ì•ˆì „í•œ ì¥ì†Œì— ì£¼ì°¨í•´ì•¼ í•©ë‹ˆë‹¤. ë¸Œë ˆì´í¬ íŒ¨ë“œì˜ ë§ˆëª¨ë‚˜ ë¸Œë ˆì´í¬ ì•¡ì²´ ë¶€ì¡±ì´ ì›ì¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¸Œë ˆì´í¬ ê²½ê³ ë“±ì´ ì¼œì ¸ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. [í˜ì´ì§€ 195-196]",
                "answer": "ë¸Œë ˆì´í¬ ì‹œìŠ¤í…œì— ì´ìƒì´ ìˆì„ ë•ŒëŠ” ì¦‰ì‹œ ì•ˆì „í•œ ì¥ì†Œì— ì£¼ì°¨í•˜ê³  ì£¼í–‰ì„ ì¤‘ë‹¨í•˜ì„¸ìš”. ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n\n1. ë¸Œë ˆì´í¬ ê²½ê³ ë“± ì ë“± ì—¬ë¶€\n2. ë¸Œë ˆì´í¬ í˜ë‹¬ì˜ ëŠë‚Œ ë³€í™”\n3. ë¸Œë ˆì´í¬ ì•¡ì²´ ë ˆë²¨\n\nì´ëŸ¬í•œ ì¦ìƒë“¤ì€ ë¸Œë ˆì´í¬ íŒ¨ë“œ ë§ˆëª¨ë‚˜ ë¸Œë ˆì´í¬ ì•¡ì²´ ë¶€ì¡±ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ˆì „ìƒ ì£¼ì˜ì‚¬í•­ìœ¼ë¡œ ì¦‰ì‹œ ë³¼ë³´ ê³µì‹ ì„œë¹„ìŠ¤ ì„¼í„°ì— ì—°ë½í•˜ì—¬ ì ê²€ì„ ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n\nğŸ“š ì°¸ê³  í˜ì´ì§€: 195-196"
            },
            {
                "query": "íƒ€ì´ì–´ ì†ŒìŒì´ ì´ìƒí•´ìš”.",
                "context": "íƒ€ì´ì–´ì—ì„œ ë¹„ì •ìƒì ì¸ ì†ŒìŒì´ ë‚˜ëŠ” ê²½ìš° ì—¬ëŸ¬ ì›ì¸ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íƒ€ì´ì–´ ê³µê¸°ì•• ë¶€ì¡±, íƒ€ì´ì–´ ë§ˆëª¨, ì´ë¬¼ì§ˆ ë¼ì„ ë“±ì´ ì£¼ìš” ì›ì¸ì…ë‹ˆë‹¤. íƒ€ì´ì–´ ê³µê¸°ì••ì„ ì •ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ê³ , íƒ€ì´ì–´ í‘œë©´ì— ì†ìƒì´ ì—†ëŠ”ì§€ ê²€ì‚¬í•˜ì„¸ìš”. [í˜ì´ì§€ 316-321]",
                "answer": "íƒ€ì´ì–´ì—ì„œ ë¹„ì •ìƒì ì¸ ì†ŒìŒì´ ë‚˜ëŠ” ê²½ìš° ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n\n1. **íƒ€ì´ì–´ ê³µê¸°ì•• í™•ì¸**: ê¶Œì¥ ì••ë ¥ì— ë§ê²Œ ì¡°ì •\n2. **íƒ€ì´ì–´ í‘œë©´ ê²€ì‚¬**: ê· ì—´, ì†ìƒ, ì´ë¬¼ì§ˆ ë¼ì„ ì—¬ë¶€ í™•ì¸\n3. **íƒ€ì´ì–´ ë§ˆëª¨ë„ ì ê²€**: íŠ¸ë ˆë“œ ê¹Šì´ í™•ì¸\n\nì´ëŸ¬í•œ ë¬¸ì œë“¤ì€ ì£¼í–‰ ì•ˆì „ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì†ŒìŒì´ ì§€ì†ë˜ê±°ë‚˜ ì•…í™”ë  ê²½ìš° ì¦‰ì‹œ ì „ë¬¸ê°€ì˜ ì ê²€ì„ ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n\nğŸ“š ì°¸ê³  í˜ì´ì§€: 316-321"
            }
        ]
        
        print("âœ… Few-shot ì˜ˆì‹œ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_few_shot_prompt(self, system_message: str, user_query: str) -> ChatPromptTemplate:
        """ì°¨ëŸ‰ ë§¤ë‰´ì–¼ ì „ìš© Few-shot í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ì˜ˆì‹œ í…œí”Œë¦¿ ì •ì˜
        example_prompt = ChatPromptTemplate.from_messages([
            ("human", "{query}"),
            ("assistant", "{answer}")
        ])
        
        # Few-shot í”„ë¡¬í”„íŠ¸ ìƒì„±
        few_shot_prompt = FewShotChatMessagePromptTemplate(
            example_prompt=example_prompt,
            examples=self.vehicle_examples
        )
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        final_prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            few_shot_prompt,
            ("human", user_query)
        ])
        
        return final_prompt
    
    def query_analyzer(self, state: AgentState) -> AgentState:
        """ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ê²°ì •"""
        query = state["query"]
        
        # Few-shot ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        analysis_system_message = """ë‹¹ì‹ ì€ ì°¨ëŸ‰ ë§¤ë‰´ì–¼ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ê²°ì •í•´ì£¼ì„¸ìš”.
        
        ë¶„ë¥˜ ê¸°ì¤€:
        - general: ì¼ë°˜ì ì¸ ì°¨ëŸ‰ ì •ë³´ ì§ˆë¬¸
        - specific: êµ¬ì²´ì ì¸ ë¶€í’ˆ/ì ˆì°¨ ì§ˆë¬¸
        - troubleshooting: ë¬¸ì œ í•´ê²° ì§ˆë¬¸
        
        ê° ì „ëµì— ë”°ë¥¸ ê²€ìƒ‰ ë°©ë²•ë„ í•¨ê»˜ ê²°ì •í•´ì£¼ì„¸ìš”."""
        
        analysis_query = f"ì§ˆë¬¸: {query}\n\nì´ ì§ˆë¬¸ì˜ ì „ëµê³¼ ê²€ìƒ‰ ë°©ë²•ì„ ê²°ì •í•´ì£¼ì„¸ìš”."
        
        # Few-shot í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì˜ˆì‹œ ë°ì´í„°ëŠ” ì§ˆë¬¸ ë¶„ì„ìš©ìœ¼ë¡œ ì‚¬ìš©)
        analysis_prompt = self._create_few_shot_prompt(analysis_system_message, analysis_query)
        
        # Few-shot í”„ë¡¬í”„íŠ¸ ì²´ì¸ ì‹¤í–‰
        try:
            analysis_chain = analysis_prompt | self.llm | StrOutputParser()
            response = analysis_chain.invoke({"query": query})
        except Exception as e:
            print(f"ì§ˆë¬¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            response = None
        
        # ì‹¤ì œë¡œëŠ” JSON íŒŒì‹± í•„ìš” - ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë¡œì§ìœ¼ë¡œ ëŒ€ì²´
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì „ëµ ê²°ì • (Re-rank ë° Compression ê³ ë ¤)
        query_lower = query.lower()
        
        # ë³µì¡í•œ ì§ˆë¬¸ íŒë‹¨ (ì—¬ëŸ¬ ìš”ì†Œ í¬í•¨)
        complexity_indicators = ['ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ë§Œì•½', 'ì–´ë–¤', 'ì–¸ì œ', 'ì™œ']
        is_complex = any(indicator in query_lower for indicator in complexity_indicators) or len(query.split()) > 8
        
        # ì¤‘ìš”ë„/ì•ˆì „ì„± íŒë‹¨
        high_importance = any(word in query_lower for word in ['ì•ˆì „', 'ìœ„í—˜', 'ê²½ê³ ', 'ë¹„ìƒ', 'ì‘ê¸‰'])
        
        if any(word in query_lower for word in ['ë¬¸ì œ', 'ì˜¤ë¥˜', 'ê²½ê³ ', 'ì•ˆë¨', 'ì´ìƒ', 'ê³ ì¥']):
            search_strategy = "troubleshooting"
            # ì•ˆì „ ê´€ë ¨ ë¬¸ì œëŠ” ê³ í’ˆì§ˆ ê²€ìƒ‰ í•„ìš”
            search_method = "cross_encoder_rerank" if high_importance else "expanded_query"
        elif any(word in query_lower for word in ['ë°©ë²•', 'ì–´ë–»ê²Œ', 'êµì²´', 'ìˆ˜ë¦¬', 'ì„¤ì¹˜']):
            search_strategy = "specific" 
            # ë³µì¡í•œ ì ˆì°¨ëŠ” ì••ì¶•ëœ ì •ë³´ í•„ìš”
            search_method = "contextual_compression" if is_complex else "multi_query"
        else:
            search_strategy = "general"
            # ì¼ë°˜ì  ì§ˆë¬¸ì€ ì „ë¬¸ìš©ì–´ í™•ì¥ ë˜ëŠ” ì¬ìˆœìœ„í™” ì‚¬ìš©
            if any(term in query_lower for term in ['íƒ€ì´ì–´', 'ë¸Œë ˆì´í¬', 'ì—”ì§„']):
                search_method = "cross_encoder_rerank"  # ì „ë¬¸ ìš©ì–´ëŠ” ì •í™•í•œ ì¬ìˆœìœ„í™” í•„ìš”
            else:
                search_method = "hybrid_semantic"
        
        state["search_strategy"] = search_strategy
        state["search_method"] = search_method  # ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€
        state["messages"].append(AIMessage(content=f"ì§ˆë¬¸ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì „ëµ: {search_strategy}, ë°©ë²•: {search_method}"))
        
        return state
    
    def search_executor(self, state: AgentState) -> AgentState:
        """ê²€ìƒ‰ ì „ëµì— ë”°ë¥¸ ì •ë³´ ìˆ˜ì§‘"""
        query = state["query"]
        strategy = state["search_strategy"]
        search_method = state.get("search_method", "hybrid_balanced")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì„¤ì •
        weight_configs = {
            "hybrid_semantic": 0.7,    # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìš°ì„ 
            "hybrid_balanced": 0.5,    # ê· í˜• ê²€ìƒ‰
            "hybrid_keyword": 0.3      # í‚¤ì›Œë“œ ê²€ìƒ‰ ìš°ì„ 
        }
        
        semantic_weight = weight_configs.get(search_method, 0.5)
        
        if strategy == "general":
            if search_method == "cross_encoder_rerank":
                # Cross-Encoder ì¬ìˆœìœ„í™” ê²€ìƒ‰
                results = cross_encoder_rerank_search.invoke({"query": query, "top_k": 5})
            elif search_method == "expanded_query":
                # ì „ë¬¸ ìš©ì–´ í™•ì¥ ê²€ìƒ‰
                results = expanded_query_search.invoke({"query": query, "top_k": 5})
            else:
                # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìš°ì„  í•˜ì´ë¸Œë¦¬ë“œ
                results = hybrid_search.invoke({
                    "query": query, 
                    "top_k": 5, 
                    "semantic_weight": semantic_weight
                })
            
        elif strategy == "specific":
            if search_method == "contextual_compression":
                # ë§¥ë½ ì••ì¶• ê²€ìƒ‰ (ë³µì¡í•œ ì ˆì°¨ì— ì í•©)
                results = contextual_compression_search.invoke({"query": query, "top_k": 5})
            elif search_method == "multi_query":
                # ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ (ë³µì¡í•œ ì§ˆë¬¸ì— ì í•©)
                results = multi_query_search.invoke({"query": query, "top_k": 5})
            else:
                # ê· í˜• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                results = hybrid_search.invoke({
                    "query": query, 
                    "top_k": 5, 
                    "semantic_weight": semantic_weight
                })
            
            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            if results and any(r.get("page", 0) > 0 for r in results):
                pages = [r.get("page", 0) for r in results if r.get("page", 0) > 0]
                context_results = page_context_search.invoke({"page_numbers": pages[:3]})
                results.extend(context_results)
            
        elif strategy == "troubleshooting":
            if search_method == "cross_encoder_rerank":
                # Cross-Encoder ì¬ìˆœìœ„í™” (ì•ˆì „ ê´€ë ¨ ë¬¸ì œì— ê³ í’ˆì§ˆ ê²€ìƒ‰)
                results = cross_encoder_rerank_search.invoke({"query": query, "top_k": 5})
            elif search_method == "expanded_query":
                # ì „ë¬¸ ìš©ì–´ í™•ì¥ ê²€ìƒ‰ (ê²½ê³ ë“± ë“± ì „ë¬¸ ìš©ì–´ í¬í•¨)
                results = expanded_query_search.invoke({"query": query, "top_k": 5})
            else:
                # í‚¤ì›Œë“œ ê²€ìƒ‰ ìš°ì„  í•˜ì´ë¸Œë¦¬ë“œ
                # ë¬¸ì œí•´ê²° ê´€ë ¨ ì¿¼ë¦¬ í™•ì¥
                expanded_query = f"ë¬¸ì œí•´ê²° ì˜¤ë¥˜ í•´ê²° {query}"
                results = hybrid_search.invoke({
                    "query": expanded_query, 
                    "top_k": 5, 
                    "semantic_weight": semantic_weight
                })
            
            # ê´€ë ¨ í˜ì´ì§€ì˜ ì»¨í…ìŠ¤íŠ¸ë„ ìˆ˜ì§‘
            if results and any(r.get("page", 0) > 0 for r in results):
                pages = [r.get("page", 0) for r in results if r.get("page", 0) > 0]
                context_results = page_context_search.invoke({"page_numbers": pages[:3]})
                results.extend(context_results)
        
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ ê· í˜• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            results = hybrid_search.invoke({
                "query": query, 
                "top_k": 5, 
                "semantic_weight": 0.5
            })
        
        # Document ê°ì²´ë¡œ ë³€í™˜
        documents = []
        for r in results:
            if isinstance(r, dict):
                documents.append(Document(
                    page_content=r.get("content", ""), 
                    metadata={
                        "page": r.get("page", 0),
                        "source": r.get("source", ""),
                        "score": r.get("score", 0.0)
                    }
                ))
            else:
                # ì´ë¯¸ Document ê°ì²´ì¸ ê²½ìš°
                documents.append(r)
        
        state["search_results"] = documents
        state["messages"].append(AIMessage(content=f"{len(documents)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. (ê²€ìƒ‰ ë°©ë²•: {search_method})"))
        
        return state
    
    def context_evaluator(self, state: AgentState) -> AgentState:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆ í‰ê°€ ë° ì¶”ê°€ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨"""
        search_results = state["search_results"]
        query = state["query"]
        
        if not search_results:
            state["need_clarification"] = True
            state["confidence_score"] = 0.0
            return state
        
        # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€
        evaluation_prompt = f"""
        ì§ˆë¬¸: {query}
        
        ê²€ìƒ‰ ê²°ê³¼: {[doc.page_content[:200] for doc in search_results[:3]]}
        
        ì´ ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µí•  ìˆ˜ ìˆëŠ”ì§€ 0-1 ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.
        0.7 ì´ìƒì´ë©´ ì¶©ë¶„, 0.7 ë¯¸ë§Œì´ë©´ ì¶”ê°€ ì •ë³´ í•„ìš”
        
        ì ìˆ˜ë§Œ ë°˜í™˜: 0.85
        """
        
        # ì‹¤ì œë¡œëŠ” LLM í˜¸ì¶œ ë° ì ìˆ˜ íŒŒì‹±
        confidence = 0.8  # ì˜ˆì‹œ
        
        state["confidence_score"] = confidence
        state["need_clarification"] = confidence < 0.7
        
        # í˜ì´ì§€ ë²ˆí˜¸ ìˆ˜ì§‘
        pages = []
        for doc in search_results:
            if "page" in doc.metadata:
                pages.append(doc.metadata["page"])
        state["page_references"] = list(set(pages))
        
        return state
    
    def answer_generator(self, state: AgentState) -> AgentState:
        """ìµœì¢… ë‹µë³€ ìƒì„±"""
        query = state["query"]
        search_results = state["search_results"]
        confidence = state["confidence_score"]
        pages = state["page_references"]
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_text = "\n\n".join([
            f"[í˜ì´ì§€ {doc.metadata.get('page', '?')}] {doc.page_content}" 
            for doc in search_results[:5]
        ])
        
        # Few-shot ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ ìƒì„±
        answer_system_message = f"""ë‹¹ì‹ ì€ ë³¼ë³´ XC60 ì°¨ëŸ‰ ë§¤ë‰´ì–¼ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        
        ë‹µë³€ ì§€ì¹¨:
        1. ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
        2. ê´€ë ¨ í˜ì´ì§€ ë²ˆí˜¸ ëª…ì‹œ
        3. ì•ˆì „ìƒ ì£¼ì˜ì‚¬í•­ ë°˜ë“œì‹œ í¬í•¨
        4. ì „ë¬¸ê°€ ìƒë‹´ ê¶Œìœ  (í•„ìš”ì‹œ)
        5. ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
        
        ì°¸ê³  ìë£Œ: {context_text}
        í™•ì‹ ë„: {confidence}"""
        
        answer_query = f"ì§ˆë¬¸: {query}\n\nìœ„ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."
        
        # Few-shot í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        answer_prompt = self._create_few_shot_prompt(answer_system_message, answer_query)
        
        # Few-shot í”„ë¡¬í”„íŠ¸ ì²´ì¸ ì‹¤í–‰
        try:
            answer_chain = answer_prompt | self.llm | StrOutputParser()
            final_answer = answer_chain.invoke({"query": query, "context_text": context_text, "confidence": confidence})
        except Exception as e:
            print(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            # í´ë°±ìœ¼ë¡œ zero-shot ë°©ì‹ ì‚¬ìš©
            fallback_prompt = f"""
            ì°¨ëŸ‰ ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
            
            ì§ˆë¬¸: {query}
            ì°¸ê³  ìë£Œ: {context_text}
            
            ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            """
            response = self.llm.invoke(fallback_prompt)
            final_answer = response.content if hasattr(response, 'content') else str(response)
        
        # í˜ì´ì§€ ì°¸ì¡° ì •ë³´ ì¶”ê°€
        if pages:
            final_answer += f"\n\nğŸ“š ì°¸ê³  í˜ì´ì§€: {', '.join(map(str, sorted(pages)))}"
        
        state["final_answer"] = final_answer
        state["context"] = context_text
        state["messages"].append(AIMessage(content=final_answer))
        
        return state
    
    def clarification_handler(self, state: AgentState) -> AgentState:
        """ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° ì²˜ë¦¬"""
        query = state["query"]
        
        clarification = f"""
        ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ë©”ë‰´ì–¼ì—ì„œ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.
        
        ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ìŒì„ í™•ì¸í•´ ë³´ì„¸ìš”:
        - ì°¨ëŸ‰ ëª¨ë¸ê³¼ ì—°ì‹
        - êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ì¦ìƒ
        - ê´€ë ¨ ë¶€í’ˆëª…
        
        ë˜ëŠ” ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì‹œê² ì–´ìš”?
        """
        
        state["final_answer"] = clarification
        state["messages"].append(AIMessage(content=clarification))
        
        return state
    
    def should_clarify(self, state: AgentState) -> str:
        """ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•œì§€ íŒë‹¨"""
        return "clarify" if state["need_clarification"] else "generate_answer"
    
    def build_graph(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(AgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_query", self.query_analyzer)
        workflow.add_node("search", self.search_executor)  
        workflow.add_node("evaluate", self.context_evaluator)
        workflow.add_node("generate_answer", self.answer_generator)
        workflow.add_node("clarify", self.clarification_handler)
        
        # ì—£ì§€ ì—°ê²°
        workflow.set_entry_point("analyze_query")
        
        workflow.add_edge("analyze_query", "search")
        workflow.add_edge("search", "evaluate")
        
        workflow.add_conditional_edges(
            "evaluate",
            self.should_clarify,
            {
                "generate_answer": "generate_answer",
                "clarify": "clarify"
            }
        )
        
        workflow.add_edge("generate_answer", END)
        workflow.add_edge("clarify", END)
        
        return workflow.compile()

# ì‚¬ìš© ì˜ˆì‹œ
def main():
    # PDF íŒŒì¼ ê²½ë¡œ ì§€ì •
    pdf_path = "/Users/1112931/llm-3/project/data/backup/kr_ko-KR_xc60_2026.pdf"
    agent = VehicleManualAgent(pdf_path=pdf_path)
    app = agent.build_graph()
    
    # Re-rank ë° Contextual Compression íš¨ê³¼ë¥¼ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤
    test_queries = [
        "íƒ€ì´ì–´ ê³µê¸°ì•• ì¸¡ì • ë° ì¡°ì • ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",  # Cross-Encoder ì¬ìˆœìœ„í™” í…ŒìŠ¤íŠ¸
        "ë¸Œë ˆì´í¬ íŒ¨ë“œ êµì²´ ì ˆì°¨ì™€ ì£¼ì˜ì‚¬í•­ ê·¸ë¦¬ê³  í•„ìš”í•œ ë„êµ¬ë“¤ì„ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”",  # Contextual Compression í…ŒìŠ¤íŠ¸
        "ì—”ì§„ ê²½ê³ ë“± ë¹„ìƒ ìƒí™© ëŒ€ì²˜ ë°©ë²•",  # Cross-Encoder (ì•ˆì „ ê´€ë ¨)
        "ì°¨ëŸ‰ ì‹œë™ì´ ì•ˆ ê±¸ë¦´ ë•Œ ì²´í¬í•  í•­ëª©ë“¤ê³¼ ë‹¨ê³„ë³„ í•´ê²° ë°©ë²•",  # Contextual Compression
        "ê²½ê³ ë“± ì ë“± ì˜ë¯¸ì™€ ëŒ€ì‘ ë°©ë²•"  # Cross-Encoder ì¬ìˆœìœ„í™”
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*50}")
        print(f"í…ŒìŠ¤íŠ¸ {i}: {query}")
        print(f"{'='*50}")
        
        initial_state = {
            "messages": [HumanMessage(content=query)],
            "query": query,
            "search_results": [],
            "context": "",
            "final_answer": "",
        "search_strategy": "",
        "search_method": "",
        "confidence_score": 0.0,
        "page_references": [],
        "need_clarification": False
        }
        
        try:
            result = app.invoke(initial_state)
            print("\nğŸ“ ìµœì¢… ë‹µë³€:")
            print(result["final_answer"])
            print(f"\nğŸ¯ ì‹ ë¢°ë„: {result['confidence_score']:.2f}")
            if result["page_references"]:
                print(f"ğŸ“š ì°¸ê³  í˜ì´ì§€: {result['page_references']}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        if i < len(test_queries):
            print("\në‹¤ìŒ í…ŒìŠ¤íŠ¸ë¥¼ 3ì´ˆ í›„ ì‹œì‘í•©ë‹ˆë‹¤...")
            import time
            time.sleep(3)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        import traceback
        traceback.print_exc()
"""
ìŒì„± ì¸ì‹ SubGraph
"""

from typing import Dict, Any, Optional
from langgraph.graph import StateGraph, START, END
import io
import wave
import tempfile
import os

from ...models.subgraph_states import SpeechRecognitionState


class DummyASR:
    """ë”ë¯¸ ASR (Automatic Speech Recognition) í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model_name = "dummy-asr-model"
        self.sample_rate = 16000
        self.chunk_size = 1024
    
    def transcribe(self, audio_data: bytes) -> str:
        """
        ìŒì„± ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë”ë¯¸ êµ¬í˜„)
        
        Args:
            audio_data: ìŒì„± ì˜¤ë””ì˜¤ ë°ì´í„° (bytes)
            
        Returns:
            str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
        """
        # ë”ë¯¸ êµ¬í˜„: ì‹¤ì œë¡œëŠ” Whisper, Google Speech-to-Text ë“±ì„ ì‚¬ìš©
        print("ğŸ¤ [DummyASR] ìŒì„± ì¸ì‹ ì¤‘...")
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©:
        # import whisper
        # model = whisper.load_model("base")
        # result = model.transcribe(audio_file_path)
        # return result["text"]
        
        # ë”ë¯¸ ì‘ë‹µ
        dummy_responses = [
            "ì—”ì§„ ì˜¤ì¼ êµì²´ ì£¼ê¸°ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
            "ë¸Œë ˆì´í¬ì—ì„œ ì†Œë¦¬ê°€ ë‚˜ëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”",
            "íƒ€ì´ì–´ ê³µê¸°ì••ì€ ì–¼ë§ˆë¡œ ë§ì¶°ì•¼ í•˜ë‚˜ìš”",
            "ì§€ê¸ˆ ìš´ì „ ì¤‘ì¸ë° ê²½ê³ ë“±ì´ ì¼œì¡Œì–´ìš”",
            "ê²¨ìš¸ì²  ì°¨ëŸ‰ ê´€ë¦¬ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
        ]
        
        import random
        dummy_text = random.choice(dummy_responses)
        print(f"ğŸ¤ [DummyASR] ì¸ì‹ëœ í…ìŠ¤íŠ¸: {dummy_text}")
        
        return dummy_text
    
    def is_audio_valid(self, audio_data: bytes) -> bool:
        """
        ìŒì„± ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        
        Args:
            audio_data: ìŒì„± ì˜¤ë””ì˜¤ ë°ì´í„°
            
        Returns:
            bool: ìœ íš¨í•œ ìŒì„± ë°ì´í„°ì¸ì§€ ì—¬ë¶€
        """
        try:
            # WAV íŒŒì¼ í—¤ë” ê²€ì‚¬
            if len(audio_data) < 44:  # WAV íŒŒì¼ ìµœì†Œ í¬ê¸°
                return False
            
            # RIFF í—¤ë” í™•ì¸
            if audio_data[:4] != b'RIFF':
                return False
            
            # WAVE í¬ë§· í™•ì¸
            if audio_data[8:12] != b'WAVE':
                return False
            
            return True
        except Exception:
            return False


class DummySTT:
    """ë”ë¯¸ STT (Speech-to-Text) í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.language = "ko-KR"  # í•œêµ­ì–´
        self.encoding = "LINEAR16"
        self.sample_rate = 16000
    
    def process_audio(self, audio_file_path: str) -> str:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë”ë¯¸ êµ¬í˜„)
        
        Args:
            audio_file_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
        """
        print(f"ğŸµ [DummySTT] ìŒì„± íŒŒì¼ ì²˜ë¦¬ ì¤‘: {audio_file_path}")
        
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©:
            # import speech_recognition as sr
            # r = sr.Recognizer()
            # with sr.AudioFile(audio_file_path) as source:
            #     audio = r.record(source)
            #     text = r.recognize_google(audio, language='ko-KR')
            #     return text
            
            # ë”ë¯¸ êµ¬í˜„: íŒŒì¼ í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë¥¸ ì‘ë‹µ ë°˜í™˜
            file_size = os.path.getsize(audio_file_path)
            
            if file_size < 1000:  # ì‘ì€ íŒŒì¼
                return "ì—”ì§„ ì˜¤ì¼ êµì²´ ì£¼ê¸°ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
            elif file_size < 5000:  # ì¤‘ê°„ íŒŒì¼
                return "ë¸Œë ˆì´í¬ì—ì„œ ì†Œë¦¬ê°€ ë‚˜ëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”"
            else:  # í° íŒŒì¼
                return "ì§€ê¸ˆ ìš´ì „ ì¤‘ì¸ë° ê²½ê³ ë“±ì´ ì¼œì¡Œì–´ìš”"
                
        except Exception as e:
            print(f"âŒ [DummySTT] ìŒì„± ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return "ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."
    
    def create_dummy_audio_file(self, duration: float = 2.0) -> str:
        """
        ë”ë¯¸ ìŒì„± íŒŒì¼ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            duration: ìŒì„± ê¸¸ì´ (ì´ˆ)
            
        Returns:
            str: ìƒì„±ëœ ìŒì„± íŒŒì¼ ê²½ë¡œ
        """
        # ì„ì‹œ WAV íŒŒì¼ ìƒì„±
        temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        temp_file_path = temp_file.name
        temp_file.close()
        
        # ë”ë¯¸ WAV íŒŒì¼ ìƒì„± (ë¬´ìŒ)
        sample_rate = 16000
        num_samples = int(sample_rate * duration)
        
        with wave.open(temp_file_path, 'w') as wav_file:
            wav_file.setnchannels(1)  # ëª¨ë…¸
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(sample_rate)
            
            # ë¬´ìŒ ë°ì´í„° ìƒì„±
            silence = b'\x00' * (num_samples * 2)
            wav_file.writeframes(silence)
        
        return temp_file_path


class SpeechRecognitionSubGraph:
    """ìŒì„± ì¸ì‹ SubGraph"""
    
    def __init__(self):
        self.asr = DummyASR()
        self.stt = DummySTT()
    
    def audio_processor(self, state: SpeechRecognitionState) -> Dict[str, Any]:
        """ìŒì„± ë°ì´í„° ì²˜ë¦¬ ë…¸ë“œ"""
        audio_data = state.get("audio_data")
        audio_file_path = state.get("audio_file_path")
        
        try:
            print("ğŸ¤ ìŒì„± ì¸ì‹ SubGraph ì‹¤í–‰ ì¤‘...")
            
            if audio_data:
                # ë°”ì´íŠ¸ ë°ì´í„°ë¡œ ì§ì ‘ ì²˜ë¦¬
                print("ğŸ“Š ë°”ì´íŠ¸ ë°ì´í„°ë¡œ ìŒì„± ì¸ì‹ ì¤‘...")
                
                if not self.asr.is_audio_valid(audio_data):
                    return {
                        "recognized_text": "",
                        "confidence": 0.0,
                        "error": "ìœ íš¨í•˜ì§€ ì•Šì€ ìŒì„± ë°ì´í„°ì…ë‹ˆë‹¤.",
                        "processing_method": "asr_bytes"
                    }
                
                recognized_text = self.asr.transcribe(audio_data)
                confidence = 0.85  # ë”ë¯¸ ì‹ ë¢°ë„
                
            elif audio_file_path:
                # íŒŒì¼ ê²½ë¡œë¡œ ì²˜ë¦¬
                print(f"ğŸ“ íŒŒì¼ ê²½ë¡œë¡œ ìŒì„± ì¸ì‹ ì¤‘: {audio_file_path}")
                
                if not os.path.exists(audio_file_path):
                    return {
                        "recognized_text": "",
                        "confidence": 0.0,
                        "error": "ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "processing_method": "stt_file"
                    }
                
                recognized_text = self.stt.process_audio(audio_file_path)
                confidence = 0.80  # ë”ë¯¸ ì‹ ë¢°ë„
                
            else:
                # ë”ë¯¸ ìŒì„± íŒŒì¼ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
                print("ğŸµ ë”ë¯¸ ìŒì„± íŒŒì¼ ìƒì„± ì¤‘...")
                dummy_file = self.stt.create_dummy_audio_file()
                recognized_text = self.stt.process_audio(dummy_file)
                confidence = 0.75
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                try:
                    os.unlink(dummy_file)
                except:
                    pass
            
            print(f"âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ: '{recognized_text}' (ì‹ ë¢°ë„: {confidence:.2f})")
            
            return {
                "recognized_text": recognized_text,
                "confidence": confidence,
                "error": None,
                "processing_method": "asr_bytes" if audio_data else "stt_file"
            }
            
        except Exception as e:
            print(f"âŒ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {str(e)}")
            return {
                "recognized_text": "",
                "confidence": 0.0,
                "error": f"ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "processing_method": "error"
            }
    
    def text_validator(self, state: SpeechRecognitionState) -> Dict[str, Any]:
        """ì¸ì‹ëœ í…ìŠ¤íŠ¸ ê²€ì¦ ë…¸ë“œ"""
        recognized_text = state.get("recognized_text", "")
        confidence = state.get("confidence", 0.0)
        
        try:
            print("ğŸ” ì¸ì‹ëœ í…ìŠ¤íŠ¸ ê²€ì¦ ì¤‘...")
            
            # ë¹ˆ í…ìŠ¤íŠ¸ ê²€ì‚¬
            if not recognized_text or recognized_text.strip() == "":
                return {
                    "is_valid": False,
                    "validation_error": "ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "final_text": ""
                }
            
            # ì‹ ë¢°ë„ ê²€ì‚¬
            if confidence < 0.3:
                return {
                    "is_valid": False,
                    "validation_error": f"ìŒì„± ì¸ì‹ ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤ ({confidence:.2f})",
                    "final_text": recognized_text
                }
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì‚¬
            if len(recognized_text) < 2:
                return {
                    "is_valid": False,
                    "validation_error": "ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.",
                    "final_text": recognized_text
                }
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            cleaned_text = recognized_text.strip()
            
            print(f"âœ… í…ìŠ¤íŠ¸ ê²€ì¦ ì™„ë£Œ: '{cleaned_text}'")
            
            return {
                "is_valid": True,
                "validation_error": None,
                "final_text": cleaned_text
            }
            
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
            return {
                "is_valid": False,
                "validation_error": f"í…ìŠ¤íŠ¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "final_text": recognized_text
            }
    
    def create_graph(self) -> StateGraph:
        """ìŒì„± ì¸ì‹ SubGraph ìƒì„±"""
        workflow = StateGraph(SpeechRecognitionState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("audio_processor", self.audio_processor)
        workflow.add_node("text_validator", self.text_validator)
        
        # ì—£ì§€ ì¶”ê°€
        workflow.set_entry_point("audio_processor")
        workflow.add_edge("audio_processor", "text_validator")
        workflow.add_edge("text_validator", END)
        
        return workflow.compile()
    
    def invoke(self, audio_data: Optional[bytes] = None, 
               audio_file_path: Optional[str] = None) -> Dict[str, Any]:
        """SubGraph ì‹¤í–‰"""
        graph = self.create_graph()
        
        initial_state = {
            "audio_data": audio_data,
            "audio_file_path": audio_file_path,
            "recognized_text": "",
            "confidence": 0.0,
            "error": None,
            "processing_method": "",
            "is_valid": False,
            "validation_error": None,
            "final_text": ""
        }
        
        return graph.invoke(initial_state)

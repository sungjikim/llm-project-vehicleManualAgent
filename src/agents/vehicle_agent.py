"""
ì°¨ëŸ‰ ë§¤ë‰´ì–¼ RAG ì—ì´ì „íŠ¸ - SubGraph ì•„í‚¤í…ì²˜
"""

from typing import Dict, Any, List
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.retrievers import EnsembleRetriever
from langgraph.graph import StateGraph, END

from ..models.states import MainAgentState
from ..config.settings import (
    DEFAULT_LLM_MODEL, DEFAULT_LLM_TEMPERATURE, DEFAULT_TOP_K, WEIGHT_CONFIGS
)
from ..retrievers.vector_retriever import VectorStoreManager
from ..retrievers.hybrid_retriever import HybridRetrieverManager
from ..retrievers.compression_retriever import CompressionRetrieverManager
from ..utils.document_loader import DocumentLoader
from ..tools.search_tools import (
    vector_store, bm25_retriever, hybrid_retriever, multi_query_retriever,
    cross_encoder_retriever, compression_retriever
)
from .subgraphs import (
    EmergencyDetectionSubGraph,
    SearchPipelineSubGraph,
    AnswerGenerationSubGraph,
    DrivingContextSubGraph,
    SpeechRecognitionSubGraph
)


class VehicleManualAgent:
    """ì°¨ëŸ‰ ë§¤ë‰´ì–¼ RAG ì—ì´ì „íŠ¸ - SubGraph ì•„í‚¤í…ì²˜"""
    
    def __init__(self, pdf_path: str):
        # LLM ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model=DEFAULT_LLM_MODEL,
            temperature=DEFAULT_LLM_TEMPERATURE
        )
        self.embeddings = OpenAIEmbeddings()
        
        # ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™”
        self.document_loader = DocumentLoader()
        
        # ê²€ìƒ‰ê¸° ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.vector_manager = VectorStoreManager(pdf_path)
        self.hybrid_manager = None
        self.compression_manager = None
        
        # ê²€ìƒ‰ ì˜µì…˜ ì„¤ì •
        self.search_options = {}
        self.rerank_compression_options = {}
        
        # SubGraph ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.emergency_subgraph = None
        self.search_subgraph = None
        self.answer_subgraph = None
        self.driving_subgraph = None
        self.speech_subgraph = None
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_system()
    
    def _initialize_system(self):
        """ì‹œìŠ¤í…œ ì „ì²´ ì´ˆê¸°í™”"""
        print("ğŸš€ ì°¨ëŸ‰ ë§¤ë‰´ì–¼ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        vector_store_instance = self.vector_manager.initialize_vector_store()
        if vector_store_instance is None:
            raise Exception("ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # ì „ì—­ ë³€ìˆ˜ ì„¤ì •
        global vector_store
        vector_store = vector_store_instance
        
        # 2. ë¬¸ì„œ ë¡œë“œ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°ìš©)
        documents = self.document_loader.load_and_split_pdf(self.vector_manager.pdf_path)
        if not documents:
            raise Exception("ë¬¸ì„œ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.hybrid_manager = HybridRetrieverManager(vector_store_instance, self.llm)
        self.hybrid_manager.initialize_bm25_retriever(documents)
        self.hybrid_manager.initialize_multi_query_retriever()
        
        # ì „ì—­ ë³€ìˆ˜ ì„¤ì •
        global bm25_retriever, multi_query_retriever
        bm25_retriever = self.hybrid_manager.get_bm25_retriever()
        multi_query_retriever = self.hybrid_manager.get_multi_query_retriever()
        
        # 4. ì••ì¶•/ì¬ìˆœìœ„í™” ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.compression_manager = CompressionRetrieverManager(
            vector_store_instance, self.embeddings, self.llm
        )
        self.compression_manager.initialize_cross_encoder_retriever()
        self.compression_manager.initialize_contextual_compression()
        
        # search_tools ëª¨ë“ˆì˜ ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        import src.tools.search_tools as search_tools
        search_tools.vector_store = vector_store
        search_tools.bm25_retriever = bm25_retriever
        search_tools.hybrid_retriever = None  # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°ëŠ” EnsembleRetrieverë¡œ ë™ì  ìƒì„±ë¨
        search_tools.multi_query_retriever = multi_query_retriever
        search_tools.cross_encoder_retriever = self.compression_manager.get_cross_encoder_retriever()
        search_tools.compression_retriever = self.compression_manager.get_compression_retriever()
        
        # 5. ê²€ìƒ‰ ì˜µì…˜ ì„¤ì •
        self._setup_search_options()
        
        # 6. SubGraph ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        self._initialize_subgraphs()
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _setup_search_options(self):
        """ê²€ìƒ‰ ì˜µì…˜ ì„¤ì •"""
        semantic_retriever = vector_store.as_retriever(search_kwargs={"k": DEFAULT_TOP_K})
        
        self.search_options = {
            "vector_only": semantic_retriever,
            "bm25_only": bm25_retriever,
            "hybrid_semantic": EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[WEIGHT_CONFIGS["hybrid_semantic"], 1 - WEIGHT_CONFIGS["hybrid_semantic"]]
            ),
            "hybrid_balanced": EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[WEIGHT_CONFIGS["hybrid_balanced"], 1 - WEIGHT_CONFIGS["hybrid_balanced"]]
            ),
            "hybrid_keyword": EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[WEIGHT_CONFIGS["hybrid_keyword"], 1 - WEIGHT_CONFIGS["hybrid_keyword"]]
            ),
            "multi_query": multi_query_retriever,
            "expanded_query": EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[WEIGHT_CONFIGS["hybrid_semantic"], 1 - WEIGHT_CONFIGS["hybrid_semantic"]]
            )
        }
        
        # ì¬ìˆœìœ„í™” ë° ì••ì¶• ì˜µì…˜
        cross_encoder_ret = self.compression_manager.get_cross_encoder_retriever()
        compression_ret = self.compression_manager.get_compression_retriever()
        
        self.rerank_compression_options = {
            "rerank_only": cross_encoder_ret,
            "compress_only": compression_ret,
            "rerank_compress_general": cross_encoder_ret,
            "rerank_compress_specific": compression_ret,
            "rerank_compress_troubleshooting": cross_encoder_ret
        }
    
    def _initialize_subgraphs(self):
        """SubGraph ì¸ìŠ¤í„´ìŠ¤ë“¤ ì´ˆê¸°í™”"""
        print("ğŸ”§ SubGraph ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        # Emergency Detection SubGraph
        self.emergency_subgraph = EmergencyDetectionSubGraph()
        
        # Search Pipeline SubGraph
        self.search_subgraph = SearchPipelineSubGraph(
            self.search_options, 
            self.rerank_compression_options
        )
        
        # Answer Generation SubGraph
        self.answer_subgraph = AnswerGenerationSubGraph()
        
        # Driving Context SubGraph
        self.driving_subgraph = DrivingContextSubGraph()
        
        # Speech Recognition SubGraph
        self.speech_subgraph = SpeechRecognitionSubGraph()
        
        print("âœ… SubGraph ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def emergency_detection_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ì‘ê¸‰ ìƒí™© ê°ì§€ ë˜í¼ ë…¸ë“œ"""
        query = state["query"]
        
        print("ğŸš¨ ì‘ê¸‰ ìƒí™© ê°ì§€ SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # Emergency Detection SubGraph ì‹¤í–‰
            emergency_result = self.emergency_subgraph.invoke(query)
            
            print(f"âœ… ì‘ê¸‰ ìƒí™© ê°ì§€ ì™„ë£Œ: {emergency_result['is_emergency']}")
            
            return {
                "is_emergency": emergency_result["is_emergency"],
                "emergency_level": emergency_result["emergency_level"],
                "emergency_score": emergency_result["emergency_score"],
                "emergency_analysis": emergency_result["emergency_analysis"],
                "search_strategy": emergency_result.get("search_strategy", ""),
                "search_method": emergency_result.get("search_method", ""),
                "compression_method": emergency_result.get("compression_method", "")
            }
            
        except Exception as e:
            print(f"âŒ ì‘ê¸‰ ìƒí™© ê°ì§€ ì˜¤ë¥˜: {str(e)}")
            return {
                "is_emergency": False,
                "emergency_level": "NORMAL",
                "emergency_score": 0.0,
                "emergency_analysis": {},
                "search_strategy": "",
                "search_method": "",
                "compression_method": ""
            }
    
    def search_pipeline_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ë˜í¼ ë…¸ë“œ"""
        query = state["query"]
        is_emergency = state.get("is_emergency", False)
        
        print("ğŸ” ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # ì‘ê¸‰ ìƒí™© ë°ì´í„° ì¤€ë¹„
            emergency_data = None
            if is_emergency:
                emergency_data = {
                    "search_strategy": state.get("search_strategy", "troubleshooting"),
                    "search_method": state.get("search_method", "hybrid_keyword"),
                    "compression_method": state.get("compression_method", "rerank_compress_troubleshooting")
                }
            
            # Search Pipeline SubGraph ì‹¤í–‰
            search_result = self.search_subgraph.invoke(
                query, 
                is_emergency=is_emergency, 
                emergency_data=emergency_data
            )
            
            print(f"âœ… ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {len(search_result['search_results'])}ê°œ ë¬¸ì„œ")
            
            return {
                "search_strategy": search_result["search_strategy"],
                "search_method": search_result["search_method"],
                "compression_method": search_result["compression_method"],
                "confidence_score": search_result["confidence_score"],
                "search_results": search_result["search_results"],
                "page_references": search_result["page_references"]
            }
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
            return {
                "search_strategy": "general",
                "search_method": "hybrid_semantic",
                "compression_method": "rerank_compress_general",
                "confidence_score": 0.5,
                "search_results": [{"content": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}],
                "page_references": []
            }
    
    def answer_generation_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ë‹µë³€ ìƒì„± ë˜í¼ ë…¸ë“œ"""
        query = state["query"]
        search_results = state.get("search_results", [])
        page_references = state.get("page_references", [])
        is_emergency = state.get("is_emergency", False)
        emergency_level = state.get("emergency_level", "NORMAL")
        
        print("ğŸ“ ë‹µë³€ ìƒì„± SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # Answer Generation SubGraph ì‹¤í–‰
            answer_result = self.answer_subgraph.invoke(
                query=query,
                search_results=search_results,
                page_references=page_references,
                is_emergency=is_emergency,
                emergency_level=emergency_level
            )
            
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(answer_result['final_answer'])}ì")
            
            return {
                "final_answer": answer_result["final_answer"],
                "confidence_score": answer_result["confidence_score"],
                "evaluation_details": answer_result["evaluation_details"]
            }
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {
                "final_answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "confidence_score": 0.0,
                "evaluation_details": None
            }
    
    def driving_context_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ì£¼í–‰ ìƒí™© ì²˜ë¦¬ ë˜í¼ ë…¸ë“œ"""
        query = state["query"]
        original_answer = state.get("final_answer", "")
        is_emergency = state.get("is_emergency", False)
        emergency_level = state.get("emergency_level", "NORMAL")
        
        print("ğŸš— ì£¼í–‰ ìƒí™© ì²˜ë¦¬ SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # Driving Context SubGraph ì‹¤í–‰
            driving_result = self.driving_subgraph.invoke(
                query=query,
                original_answer=original_answer,
                is_emergency=is_emergency,
                emergency_level=emergency_level
            )
            
            print(f"âœ… ì£¼í–‰ ìƒí™© ì²˜ë¦¬ ì™„ë£Œ: {driving_result['is_driving']}")
            
            return {
                "is_driving": driving_result["is_driving"],
                "driving_confidence": driving_result["driving_confidence"],
                "driving_indicators": driving_result["driving_indicators"],
                "driving_urgency": driving_result["driving_urgency"],
                "compression_needed": driving_result["compression_needed"],
                "compressed_answer": driving_result["compressed_answer"],
                "final_answer": driving_result["final_answer"]
            }
            
        except Exception as e:
            print(f"âŒ ì£¼í–‰ ìƒí™© ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return {
                "is_driving": False,
                "driving_confidence": 0.0,
                "driving_indicators": [],
                "driving_urgency": "normal",
                "compression_needed": False,
                "compressed_answer": "",
                "final_answer": original_answer
            }
    
    def speech_recognition_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ìŒì„± ì¸ì‹ ë˜í¼ ë…¸ë“œ"""
        audio_data = state.get("audio_data")
        audio_file_path = state.get("audio_file_path")
        existing_query = state.get("query", "")
        
        # ì´ë¯¸ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ìŒì„± ì¸ì‹ ê±´ë„ˆë›°ê¸°
        if existing_query and existing_query.strip():
            print("ğŸ“ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ê°ì§€ - ìŒì„± ì¸ì‹ ê±´ë„ˆë›°ê¸°")
            return {
                "recognized_text": "",
                "speech_confidence": 0.0,
                "speech_error": None,
                "query": existing_query  # ê¸°ì¡´ ì¿¼ë¦¬ ìœ ì§€
            }
        
        print("ğŸ¤ ìŒì„± ì¸ì‹ SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # Speech Recognition SubGraph ì‹¤í–‰
            speech_result = self.speech_subgraph.invoke(
                audio_data=audio_data,
                audio_file_path=audio_file_path
            )
            
            print(f"âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ: '{speech_result['final_text']}'")
            
            return {
                "recognized_text": speech_result["final_text"],
                "speech_confidence": speech_result["confidence"],
                "speech_error": speech_result["error"],
                "query": speech_result["final_text"]  # ì¸ì‹ëœ í…ìŠ¤íŠ¸ë¥¼ ì¿¼ë¦¬ë¡œ ì„¤ì •
            }
            
        except Exception as e:
            print(f"âŒ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {str(e)}")
            return {
                "recognized_text": "",
                "speech_confidence": 0.0,
                "speech_error": f"ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "query": ""
            }
    
    def create_graph(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± - SubGraph ì•„í‚¤í…ì²˜"""
        workflow = StateGraph(MainAgentState)
        
        # ë…¸ë“œ ì¶”ê°€ (SubGraph ë˜í¼ë“¤)
        workflow.add_node("speech_recognition", self.speech_recognition_wrapper)
        workflow.add_node("emergency_detection", self.emergency_detection_wrapper)
        workflow.add_node("search_pipeline", self.search_pipeline_wrapper)
        workflow.add_node("answer_generation", self.answer_generation_wrapper)
        workflow.add_node("driving_context", self.driving_context_wrapper)
        
        # ì—£ì§€ ì¶”ê°€ (ìŒì„± ì¸ì‹ â†’ ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°)
        workflow.set_entry_point("speech_recognition")
        workflow.add_edge("speech_recognition", "emergency_detection")
        workflow.add_edge("emergency_detection", "search_pipeline")
        workflow.add_edge("search_pipeline", "answer_generation")
        workflow.add_edge("answer_generation", "driving_context")
        workflow.add_edge("driving_context", END)
        
        return workflow.compile()
    
    def create_emergency_fast_path(self) -> StateGraph:
        """ì‘ê¸‰ ìƒí™© ì „ìš© ë¹ ë¥¸ ê²½ë¡œ - ìµœì†Œí•œì˜ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ ì‘ë‹µ"""
        workflow = StateGraph(MainAgentState)
        
        # ì‘ê¸‰ ìƒí™©ì—ì„œëŠ” ìµœì†Œí•œì˜ ë…¸ë“œë§Œ ì‹¤í–‰
        workflow.add_node("speech_recognition", self.speech_recognition_wrapper)
        workflow.add_node("emergency_detection", self.emergency_detection_wrapper)
        workflow.add_node("emergency_search", self.emergency_search_wrapper)
        workflow.add_node("emergency_answer", self.emergency_answer_wrapper)
        
        # ë¹ ë¥¸ ê²½ë¡œ: ìŒì„±ì¸ì‹ â†’ ì‘ê¸‰ê°ì§€ â†’ ê°„ì†Œê²€ìƒ‰ â†’ ê°„ì†Œë‹µë³€
        workflow.set_entry_point("speech_recognition")
        workflow.add_edge("speech_recognition", "emergency_detection")
        workflow.add_edge("emergency_detection", "emergency_search")
        workflow.add_edge("emergency_search", "emergency_answer")
        workflow.add_edge("emergency_answer", END)
        
        return workflow.compile()
    
    def emergency_search_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ì‘ê¸‰ ìƒí™© ì „ìš© ê°„ì†Œí™”ëœ ê²€ìƒ‰ - ì†ë„ ìš°ì„ """
        query = state["query"]
        
        print("ğŸš¨ ì‘ê¸‰ ìƒí™© ê²€ìƒ‰ - ë¹ ë¥¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰")
        
        try:
            # ì‘ê¸‰ ìƒí™©ì—ì„œëŠ” ê°€ì¥ ë¹ ë¥¸ BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš©
            retriever = self.search_options["bm25_only"]
            docs = retriever.invoke(query)
            
            # ìµœëŒ€ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš© (ì†ë„ ìš°ì„ )
            search_results = [
                {
                    "content": doc.page_content,
                    "page": doc.metadata.get("page", 0),
                    "source": doc.metadata.get("source", ""),
                    "score": 1.0  # ì‘ê¸‰ ìƒí™©ì—ì„œëŠ” ì ìˆ˜ ê³„ì‚° ìƒëµ
                }
                for doc in docs[:3]  # 3ê°œë¡œ ì œí•œ
            ]
            
            # í˜ì´ì§€ ì°¸ì¡° ì¶”ì¶œ
            page_references = list(set([
                result.get("page", 0) for result in search_results if result.get("page", 0) > 0
            ]))
            
            print(f"âš¡ ì‘ê¸‰ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ë¬¸ì„œ, {len(page_references)}ê°œ í˜ì´ì§€")
            
            return {
                "search_strategy": "emergency_fast",
                "search_method": "bm25_only",
                "compression_method": "none",  # ì••ì¶• ìƒëµ
                "confidence_score": 0.9,  # ê³ ì • ì‹ ë¢°ë„
                "search_results": search_results,
                "page_references": page_references
            }
            
        except Exception as e:
            print(f"âŒ ì‘ê¸‰ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return {
                "search_strategy": "emergency_fast",
                "search_method": "bm25_only",
                "compression_method": "none",
                "confidence_score": 0.5,
                "search_results": [{"content": f"ì‘ê¸‰ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", "page": 0, "score": 0.0}],
                "page_references": []
            }
    
    def emergency_answer_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ì‘ê¸‰ ìƒí™© ì „ìš© ê°„ì†Œí™”ëœ ë‹µë³€ ìƒì„± - ì†ë„ ìš°ì„ """
        query = state["query"]
        search_results = state.get("search_results", [])
        page_references = state.get("page_references", [])
        emergency_level = state.get("emergency_level", "HIGH")
        
        print(f"ğŸš¨ ì‘ê¸‰ ë‹µë³€ ìƒì„± - {emergency_level} ìˆ˜ì¤€")
        
        try:
            # ê°„ì†Œí™”ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ìµœëŒ€ 2ê°œ ë¬¸ì„œ, ê° 200ìë§Œ)
            context_parts = []
            for i, result in enumerate(search_results[:2], 1):
                content = result.get("content", "")
                page = result.get("page", 0)
                
                # 200ìë¡œ ì œí•œ
                if len(content) > 200:
                    content = content[:200] + "..."
                
                if page > 0:
                    context_parts.append(f"[ì°¸ê³  {i}] (í˜ì´ì§€ {page})\n{content}")
                else:
                    context_parts.append(f"[ì°¸ê³  {i}]\n{content}")
            
            context = "\n\n".join(context_parts)
            
            # ê°„ì†Œí™”ëœ ì‘ê¸‰ í”„ë¡¬í”„íŠ¸ (ChatPromptTemplate ì‚¬ìš©)
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            
            emergency_prompt = ChatPromptTemplate.from_messages([
                ("system", """ì‘ê¸‰ ìƒí™©ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•ˆì „ ì¡°ì¹˜ë§Œ ê°„ë‹¨íˆ ì œì‹œí•˜ì„¸ìš”.
        
ë‹µë³€ í˜•ì‹:
ğŸš¨ ì¦‰ì‹œ ì¡°ì¹˜: [í•µì‹¬ í–‰ë™ 1-2ê°œ]
âš ï¸ ì•ˆì „ ê²½ê³ : [ì¤‘ìš”í•œ ì£¼ì˜ì‚¬í•­]
ğŸ“ ì—°ë½ì²˜: [í•„ìš”ì‹œ ì‘ê¸‰ ì„œë¹„ìŠ¤]

ì‘ê¸‰ ìˆ˜ì¤€ë³„ ëŒ€ì‘:
- CRITICAL: ìƒëª… ìœ„í—˜, ì¦‰ì‹œ 119 ì‹ ê³ 
- HIGH: ì¦‰ì‹œ ì•ˆì „ ì¡°ì¹˜ í•„ìš”
- MEDIUM: ì‹ ì†í•œ ëŒ€ì‘ í•„ìš”
- LOW: ì£¼ì˜ í•„ìš”"""),
                ("human", """ì§ˆë¬¸: {query}
ì°¸ê³  ì •ë³´: {context}
ì‘ê¸‰ ìˆ˜ì¤€: {emergency_level}

ë‹µë³€:""")
            ])
            
            # LLM í˜¸ì¶œ (ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸)
            answer_chain = emergency_prompt | self.llm | StrOutputParser()
            final_answer = answer_chain.invoke({
                "query": query, 
                "context": context,
                "emergency_level": emergency_level
            })
            
            # ì‘ê¸‰ ìƒí™© í—¤ë” ì¶”ê°€
            emergency_icons = {
                "CRITICAL": "ğŸ”¥",
                "HIGH": "ğŸš¨", 
                "MEDIUM": "âš ï¸",
                "LOW": "ğŸ”"
            }
            icon = emergency_icons.get(emergency_level, "ğŸš¨")
            emergency_header = f"{icon} **{emergency_level} ì‘ê¸‰ ìƒí™©**\n\n"
            
            # ì‘ê¸‰ ìƒí™© ê²½ê³  ì¶”ê°€
            if emergency_level == "CRITICAL":
                emergency_warning = "\n\nğŸš¨ **ìƒëª… ìœ„í—˜ ìƒí™©ì…ë‹ˆë‹¤. ì¦‰ì‹œ 119ì— ì‹ ê³ í•˜ì„¸ìš”.**"
            elif emergency_level == "HIGH":
                emergency_warning = "\n\nâš ï¸ **ì¦‰ì‹œ ì•ˆì „ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì „ë¬¸ê°€ì—ê²Œ ì—°ë½í•˜ì„¸ìš”.**"
            else:
                emergency_warning = "\n\nâš ï¸ **ì‹ ì†í•œ ëŒ€ì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.**"
            
            final_answer_with_header = emergency_header + final_answer + emergency_warning
            
            print(f"âœ… ì‘ê¸‰ ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(final_answer_with_header)}ì")
            
            return {
                "final_answer": final_answer_with_header,
                "confidence_score": 0.85,  # ê³ ì • ì‹ ë¢°ë„ (í‰ê°€ ìƒëµ)
                "evaluation_details": None  # í‰ê°€ ìƒëµ
            }
            
        except Exception as e:
            print(f"âŒ ì‘ê¸‰ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {
                "final_answer": f"ğŸš¨ ì‘ê¸‰ ìƒí™© ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nâš ï¸ ì¦‰ì‹œ ì•ˆì „í•œ ê³³ì— ì •ì°¨í•˜ê³  ì „ë¬¸ê°€ì—ê²Œ ì—°ë½í•˜ì„¸ìš”.",
                "confidence_score": 0.5,
                "evaluation_details": None
            }
    
    def query(self, user_query: str = None, audio_data: bytes = None, 
              audio_file_path: str = None, callbacks=None) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ - ì‘ê¸‰ ìƒí™© ê°ì§€ í›„ ì ì ˆí•œ ì›Œí¬í”Œë¡œìš° ì„ íƒ"""
        try:
            # 1. ë¨¼ì € ë¹ ë¥¸ ì‘ê¸‰ ìƒí™© ê°ì§€ (í…ìŠ¤íŠ¸ ì¿¼ë¦¬ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
            if user_query and user_query.strip():
                # LLM ê¸°ë°˜ ì‘ê¸‰ ìƒí™© ê°ì§€ ì‚¬ìš©
                from ..utils.llm_emergency_detector import LLMEmergencyDetector
                llm_emergency_detector = LLMEmergencyDetector()
                emergency_result = llm_emergency_detector.detect_emergency(user_query)
                
                # CRITICAL ë˜ëŠ” HIGH ì‘ê¸‰ ìƒí™©ì´ë©´ ë¹ ë¥¸ ê²½ë¡œ ì‚¬ìš©
                if emergency_result["is_emergency"] and emergency_result["priority_level"] in ["CRITICAL", "HIGH"]:
                    print(f"ğŸš¨ ì‘ê¸‰ ìƒí™© ê°ì§€ ({emergency_result['priority_level']}) - ë¹ ë¥¸ ê²½ë¡œ ì‹¤í–‰")
                    graph = self.create_emergency_fast_path()
                    use_emergency_path = True
                else:
                    print("ğŸ“ ì¼ë°˜ ì§ˆë¬¸ - ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰")
                    graph = self.create_graph()
                    use_emergency_path = False
            else:
                # ìŒì„± ì¸ì‹ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
                print("ğŸ¤ ìŒì„± ì…ë ¥ - ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰")
                graph = self.create_graph()
                use_emergency_path = False
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {
                "messages": [],
                "query": user_query or "",  # í…ìŠ¤íŠ¸ ì¿¼ë¦¬ (ìŒì„± ì¸ì‹ ì‹œ ë®ì–´ì”Œì›Œì§)
                "search_results": [],
                "context": "",
                "final_answer": "",
                "search_strategy": "",
                "search_method": "",
                "confidence_score": 0.0,
                "page_references": [],
                "need_clarification": False,
                # ì‘ê¸‰ ìƒí™© ê´€ë ¨ ì´ˆê¸°ê°’
                "is_emergency": False,
                "emergency_level": "NORMAL",
                "emergency_score": 0.0,
                "emergency_analysis": {},
                "compression_method": "",
                # ì£¼í–‰ ìƒí™© ê´€ë ¨ ì´ˆê¸°ê°’
                "is_driving": False,
                "driving_confidence": 0.0,
                "driving_indicators": [],
                "driving_urgency": "normal",
                "compression_needed": False,
                "compressed_answer": "",
                # ìŒì„± ì¸ì‹ ê´€ë ¨ ì´ˆê¸°ê°’
                "audio_data": audio_data,
                "audio_file_path": audio_file_path,
                "recognized_text": "",
                "speech_confidence": 0.0,
                "speech_error": None,
                # í‰ê°€ ê´€ë ¨
                "evaluation_details": None
            }
            
            # ì‘ê¸‰ ê²½ë¡œ ì‚¬ìš© ì‹œ ì‘ê¸‰ ìƒí™© ì •ë³´ ë¯¸ë¦¬ ì„¤ì •
            if use_emergency_path and user_query:
                initial_state.update({
                    "is_emergency": True,
                    "emergency_level": emergency_result["priority_level"],
                    "emergency_score": emergency_result["total_score"],
                    "emergency_analysis": emergency_result
                })
            
            # ì½œë°±ì´ ìˆìœ¼ë©´ ì„¤ì •ì— í¬í•¨
            config = {}
            if callbacks:
                config["callbacks"] = callbacks
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            result = graph.invoke(initial_state, config=config)
            
            return result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            return f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

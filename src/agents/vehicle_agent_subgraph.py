"""
ì°¨ëŸ‰ ë§¤ë‰´ì–¼ RAG ì—ì´ì „íŠ¸ - SubGraph ì•„í‚¤í…ì²˜
"""

from typing import Dict, Any, List
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.retrievers import EnsembleRetriever
from langgraph.graph import StateGraph, END

from ..models.subgraph_states import MainAgentState
from ..config.settings import (
    DEFAULT_LLM_MODEL, DEFAULT_LLM_TEMPERATURE, DEFAULT_TOP_K, WEIGHT_CONFIGS
)
from ..retrievers.vector_retriever import VectorStoreManager
from ..retrievers.hybrid_retriever import HybridRetrieverManager
from ..retrievers.compression_retriever import CompressionRetrieverManager
from ..utils.document_loader import DocumentLoader
from ..tools.search_tools import (
    vector_store, bm25_retriever, hybrid_retriever, multi_query_retriever,
    cross_encoder_retriever, compression_retriever
)
from .subgraphs import (
    EmergencyDetectionSubGraph,
    SearchPipelineSubGraph,
    AnswerGenerationSubGraph,
    DrivingContextSubGraph
)


class VehicleManualAgentSubGraph:
    """ì°¨ëŸ‰ ë§¤ë‰´ì–¼ RAG ì—ì´ì „íŠ¸ - SubGraph ì•„í‚¤í…ì²˜"""
    
    def __init__(self, pdf_path: str):
        # LLM ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model=DEFAULT_LLM_MODEL,
            temperature=DEFAULT_LLM_TEMPERATURE
        )
        self.embeddings = OpenAIEmbeddings()
        
        # ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™”
        self.document_loader = DocumentLoader()
        
        # ê²€ìƒ‰ê¸° ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.vector_manager = VectorStoreManager(pdf_path)
        self.hybrid_manager = None
        self.compression_manager = None
        
        # ê²€ìƒ‰ ì˜µì…˜ ì„¤ì •
        self.search_options = {}
        self.rerank_compression_options = {}
        
        # SubGraph ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.emergency_subgraph = None
        self.search_subgraph = None
        self.answer_subgraph = None
        self.driving_subgraph = None
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_system()
    
    def _initialize_system(self):
        """ì‹œìŠ¤í…œ ì „ì²´ ì´ˆê¸°í™”"""
        print("ğŸš€ ì°¨ëŸ‰ ë§¤ë‰´ì–¼ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        vector_store_instance = self.vector_manager.initialize_vector_store()
        if vector_store_instance is None:
            raise Exception("ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # ì „ì—­ ë³€ìˆ˜ ì„¤ì •
        global vector_store
        vector_store = vector_store_instance
        
        # 2. ë¬¸ì„œ ë¡œë“œ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°ìš©)
        documents = self.document_loader.load_and_split_pdf(self.vector_manager.pdf_path)
        if not documents:
            raise Exception("ë¬¸ì„œ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.hybrid_manager = HybridRetrieverManager(vector_store_instance, self.llm)
        self.hybrid_manager.initialize_bm25_retriever(documents)
        self.hybrid_manager.initialize_multi_query_retriever()
        
        # ì „ì—­ ë³€ìˆ˜ ì„¤ì •
        global bm25_retriever, multi_query_retriever
        bm25_retriever = self.hybrid_manager.get_bm25_retriever()
        multi_query_retriever = self.hybrid_manager.get_multi_query_retriever()
        
        # 4. ì••ì¶•/ì¬ìˆœìœ„í™” ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.compression_manager = CompressionRetrieverManager(
            vector_store_instance, self.embeddings, self.llm
        )
        self.compression_manager.initialize_cross_encoder_retriever()
        self.compression_manager.initialize_contextual_compression()
        
        # search_tools ëª¨ë“ˆì˜ ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        import src.tools.search_tools as search_tools
        search_tools.vector_store = vector_store
        search_tools.bm25_retriever = bm25_retriever
        search_tools.hybrid_retriever = None  # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°ëŠ” EnsembleRetrieverë¡œ ë™ì  ìƒì„±ë¨
        search_tools.multi_query_retriever = multi_query_retriever
        search_tools.cross_encoder_retriever = self.compression_manager.get_cross_encoder_retriever()
        search_tools.compression_retriever = self.compression_manager.get_compression_retriever()
        
        # 5. ê²€ìƒ‰ ì˜µì…˜ ì„¤ì •
        self._setup_search_options()
        
        # 6. SubGraph ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        self._initialize_subgraphs()
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _setup_search_options(self):
        """ê²€ìƒ‰ ì˜µì…˜ ì„¤ì •"""
        semantic_retriever = vector_store.as_retriever(search_kwargs={"k": DEFAULT_TOP_K})
        
        self.search_options = {
            "vector_only": semantic_retriever,
            "bm25_only": bm25_retriever,
            "hybrid_semantic": EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[WEIGHT_CONFIGS["hybrid_semantic"], 1 - WEIGHT_CONFIGS["hybrid_semantic"]]
            ),
            "hybrid_balanced": EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[WEIGHT_CONFIGS["hybrid_balanced"], 1 - WEIGHT_CONFIGS["hybrid_balanced"]]
            ),
            "hybrid_keyword": EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[WEIGHT_CONFIGS["hybrid_keyword"], 1 - WEIGHT_CONFIGS["hybrid_keyword"]]
            ),
            "multi_query": multi_query_retriever,
            "expanded_query": EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[WEIGHT_CONFIGS["hybrid_semantic"], 1 - WEIGHT_CONFIGS["hybrid_semantic"]]
            )
        }
        
        # ì¬ìˆœìœ„í™” ë° ì••ì¶• ì˜µì…˜
        cross_encoder_ret = self.compression_manager.get_cross_encoder_retriever()
        compression_ret = self.compression_manager.get_compression_retriever()
        
        self.rerank_compression_options = {
            "rerank_only": cross_encoder_ret,
            "compress_only": compression_ret,
            "rerank_compress_general": cross_encoder_ret,
            "rerank_compress_specific": compression_ret,
            "rerank_compress_troubleshooting": cross_encoder_ret
        }
    
    def _initialize_subgraphs(self):
        """SubGraph ì¸ìŠ¤í„´ìŠ¤ë“¤ ì´ˆê¸°í™”"""
        print("ğŸ”§ SubGraph ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        # Emergency Detection SubGraph
        self.emergency_subgraph = EmergencyDetectionSubGraph()
        
        # Search Pipeline SubGraph
        self.search_subgraph = SearchPipelineSubGraph(
            self.search_options, 
            self.rerank_compression_options
        )
        
        # Answer Generation SubGraph
        self.answer_subgraph = AnswerGenerationSubGraph()
        
        # Driving Context SubGraph
        self.driving_subgraph = DrivingContextSubGraph()
        
        print("âœ… SubGraph ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def emergency_detection_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ì‘ê¸‰ ìƒí™© ê°ì§€ ë˜í¼ ë…¸ë“œ"""
        query = state["query"]
        
        print("ğŸš¨ ì‘ê¸‰ ìƒí™© ê°ì§€ SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # Emergency Detection SubGraph ì‹¤í–‰
            emergency_result = self.emergency_subgraph.invoke(query)
            
            print(f"âœ… ì‘ê¸‰ ìƒí™© ê°ì§€ ì™„ë£Œ: {emergency_result['is_emergency']}")
            
            return {
                "is_emergency": emergency_result["is_emergency"],
                "emergency_level": emergency_result["emergency_level"],
                "emergency_score": emergency_result["emergency_score"],
                "emergency_analysis": emergency_result["emergency_analysis"],
                "search_strategy": emergency_result.get("search_strategy", ""),
                "search_method": emergency_result.get("search_method", ""),
                "compression_method": emergency_result.get("compression_method", "")
            }
            
        except Exception as e:
            print(f"âŒ ì‘ê¸‰ ìƒí™© ê°ì§€ ì˜¤ë¥˜: {str(e)}")
            return {
                "is_emergency": False,
                "emergency_level": "NORMAL",
                "emergency_score": 0.0,
                "emergency_analysis": {},
                "search_strategy": "",
                "search_method": "",
                "compression_method": ""
            }
    
    def search_pipeline_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ë˜í¼ ë…¸ë“œ"""
        query = state["query"]
        is_emergency = state.get("is_emergency", False)
        
        print("ğŸ” ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # ì‘ê¸‰ ìƒí™© ë°ì´í„° ì¤€ë¹„
            emergency_data = None
            if is_emergency:
                emergency_data = {
                    "search_strategy": state.get("search_strategy", "troubleshooting"),
                    "search_method": state.get("search_method", "hybrid_keyword"),
                    "compression_method": state.get("compression_method", "rerank_compress_troubleshooting")
                }
            
            # Search Pipeline SubGraph ì‹¤í–‰
            search_result = self.search_subgraph.invoke(
                query, 
                is_emergency=is_emergency, 
                emergency_data=emergency_data
            )
            
            print(f"âœ… ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {len(search_result['search_results'])}ê°œ ë¬¸ì„œ")
            
            return {
                "search_strategy": search_result["search_strategy"],
                "search_method": search_result["search_method"],
                "compression_method": search_result["compression_method"],
                "confidence_score": search_result["confidence_score"],
                "search_results": search_result["search_results"],
                "page_references": search_result["page_references"]
            }
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
            return {
                "search_strategy": "general",
                "search_method": "hybrid_semantic",
                "compression_method": "rerank_compress_general",
                "confidence_score": 0.5,
                "search_results": [{"content": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "page": 0, "score": 0.0}],
                "page_references": []
            }
    
    def answer_generation_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ë‹µë³€ ìƒì„± ë˜í¼ ë…¸ë“œ"""
        query = state["query"]
        search_results = state.get("search_results", [])
        page_references = state.get("page_references", [])
        is_emergency = state.get("is_emergency", False)
        emergency_level = state.get("emergency_level", "NORMAL")
        
        print("ğŸ“ ë‹µë³€ ìƒì„± SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # Answer Generation SubGraph ì‹¤í–‰
            answer_result = self.answer_subgraph.invoke(
                query=query,
                search_results=search_results,
                page_references=page_references,
                is_emergency=is_emergency,
                emergency_level=emergency_level
            )
            
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(answer_result['final_answer'])}ì")
            
            return {
                "final_answer": answer_result["final_answer"],
                "confidence_score": answer_result["confidence_score"],
                "evaluation_details": answer_result["evaluation_details"]
            }
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {
                "final_answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "confidence_score": 0.0,
                "evaluation_details": None
            }
    
    def driving_context_wrapper(self, state: MainAgentState) -> Dict[str, Any]:
        """ì£¼í–‰ ìƒí™© ì²˜ë¦¬ ë˜í¼ ë…¸ë“œ"""
        query = state["query"]
        original_answer = state.get("final_answer", "")
        is_emergency = state.get("is_emergency", False)
        emergency_level = state.get("emergency_level", "NORMAL")
        
        print("ğŸš— ì£¼í–‰ ìƒí™© ì²˜ë¦¬ SubGraph ì‹¤í–‰ ì¤‘...")
        
        try:
            # Driving Context SubGraph ì‹¤í–‰
            driving_result = self.driving_subgraph.invoke(
                query=query,
                original_answer=original_answer,
                is_emergency=is_emergency,
                emergency_level=emergency_level
            )
            
            print(f"âœ… ì£¼í–‰ ìƒí™© ì²˜ë¦¬ ì™„ë£Œ: {driving_result['is_driving']}")
            
            return {
                "is_driving": driving_result["is_driving"],
                "driving_confidence": driving_result["driving_confidence"],
                "driving_indicators": driving_result["driving_indicators"],
                "driving_urgency": driving_result["driving_urgency"],
                "compression_needed": driving_result["compression_needed"],
                "compressed_answer": driving_result["compressed_answer"],
                "final_answer": driving_result["final_answer"]
            }
            
        except Exception as e:
            print(f"âŒ ì£¼í–‰ ìƒí™© ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return {
                "is_driving": False,
                "driving_confidence": 0.0,
                "driving_indicators": [],
                "driving_urgency": "normal",
                "compression_needed": False,
                "compressed_answer": "",
                "final_answer": original_answer
            }
    
    def create_graph(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± - SubGraph ì•„í‚¤í…ì²˜"""
        workflow = StateGraph(MainAgentState)
        
        # ë…¸ë“œ ì¶”ê°€ (SubGraph ë˜í¼ë“¤)
        workflow.add_node("emergency_detection", self.emergency_detection_wrapper)
        workflow.add_node("search_pipeline", self.search_pipeline_wrapper)
        workflow.add_node("answer_generation", self.answer_generation_wrapper)
        workflow.add_node("driving_context", self.driving_context_wrapper)
        
        # ì—£ì§€ ì¶”ê°€ (ìˆœì°¨ì  ì‹¤í–‰)
        workflow.set_entry_point("emergency_detection")
        workflow.add_edge("emergency_detection", "search_pipeline")
        workflow.add_edge("search_pipeline", "answer_generation")
        workflow.add_edge("answer_generation", "driving_context")
        workflow.add_edge("driving_context", END)
        
        return workflow.compile()
    
    def query(self, user_query: str, callbacks=None) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ - SubGraph ì•„í‚¤í…ì²˜"""
        try:
            graph = self.create_graph()
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {
                "messages": [],
                "query": user_query,
                "search_results": [],
                "context": "",
                "final_answer": "",
                "search_strategy": "",
                "search_method": "",
                "confidence_score": 0.0,
                "page_references": [],
                "need_clarification": False,
                # ì‘ê¸‰ ìƒí™© ê´€ë ¨ ì´ˆê¸°ê°’
                "is_emergency": False,
                "emergency_level": "NORMAL",
                "emergency_score": 0.0,
                "emergency_analysis": {},
                "compression_method": "",
                # ì£¼í–‰ ìƒí™© ê´€ë ¨ ì´ˆê¸°ê°’
                "is_driving": False,
                "driving_confidence": 0.0,
                "driving_indicators": [],
                "driving_urgency": "normal",
                "compression_needed": False,
                "compressed_answer": "",
                # í‰ê°€ ê´€ë ¨
                "evaluation_details": None
            }
            
            # ì½œë°±ì´ ìˆìœ¼ë©´ ì„¤ì •ì— í¬í•¨
            config = {}
            if callbacks:
                config["callbacks"] = callbacks
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            result = graph.invoke(initial_state, config=config)
            
            return result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            return f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

"""
ë‹µë³€ í’ˆì§ˆ í‰ê°€ ìœ í‹¸ë¦¬í‹°
"""

import re
from typing import Dict, List, Any


class AnswerEvaluator:
    """ë‹µë³€ í’ˆì§ˆ ë° ì‹ ë¢°ì„± í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # í’ˆì§ˆ í‰ê°€ ê¸°ì¤€
        self.quality_criteria = {
            'page_reference': {
                'weight': 0.25,
                'description': 'í˜ì´ì§€ ì°¸ì¡° ì •ë³´ í¬í•¨ ì—¬ë¶€'
            },
            'specific_info': {
                'weight': 0.30,
                'description': 'êµ¬ì²´ì  ì •ë³´ (ìˆ˜ì¹˜, ì ˆì°¨) í¬í•¨ ì—¬ë¶€'
            },
            'answer_length': {
                'weight': 0.20,
                'description': 'ì ì ˆí•œ ë‹µë³€ ê¸¸ì´'
            },
            'safety_info': {
                'weight': 0.15,
                'description': 'ì•ˆì „ ê´€ë ¨ ì •ë³´ í¬í•¨ ì—¬ë¶€'
            },
            'error_free': {
                'weight': 0.10,
                'description': 'ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ'
            }
        }
        
        # ì‹ ë¢°ì„± í‚¤ì›Œë“œ
        self.specific_keywords = [
            'ì••ë ¥', 'ì£¼ê¸°', 'ë°©ë²•', 'ì ˆì°¨', 'ë‹¨ê³„', 'ì˜¨ë„', 'ìš©ëŸ‰', 
            'ê±°ë¦¬', 'ì‹œê°„', 'ì†ë„', 'km', 'psi', 'bar', 'ë¦¬í„°', 'ë„'
        ]
        
        self.safety_keywords = [
            'ì•ˆì „', 'ì£¼ì˜', 'ê²½ê³ ', 'ìœ„í—˜', 'ì¦‰ì‹œ', 'ì „ë¬¸ê°€', 'ì„œë¹„ìŠ¤ì„¼í„°',
            'ì ê²€', 'í™•ì¸', 'ì¡°ì¹˜', 'ëŒ€ì²˜'
        ]
    
    def evaluate_answer(self, question: str, answer: str, search_results: List[Dict] = None) -> Dict[str, Any]:
        """ë‹µë³€ ì¢…í•© í‰ê°€"""
        scores = {}
        
        # 1. í˜ì´ì§€ ì°¸ì¡° í‰ê°€
        scores['page_reference'] = self._evaluate_page_reference(answer)
        
        # 2. êµ¬ì²´ì  ì •ë³´ í‰ê°€
        scores['specific_info'] = self._evaluate_specific_info(answer)
        
        # 3. ë‹µë³€ ê¸¸ì´ í‰ê°€
        scores['answer_length'] = self._evaluate_answer_length(answer)
        
        # 4. ì•ˆì „ ì •ë³´ í‰ê°€
        scores['safety_info'] = self._evaluate_safety_info(answer)
        
        # 5. ì˜¤ë¥˜ ì—†ìŒ í‰ê°€
        scores['error_free'] = self._evaluate_error_free(answer)
        
        # 6. ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ (ìƒˆë¡œ ì¶”ê°€)
        if search_results:
            scores['search_quality'] = self._evaluate_search_quality(search_results)
            # ê°€ì¤‘ì¹˜ ì¬ì¡°ì • (ê²€ìƒ‰ í’ˆì§ˆ í¬í•¨)
            self.quality_criteria['search_quality'] = {
                'weight': 0.15,
                'description': 'ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ë° ê´€ë ¨ì„±'
            }
            # ê¸°ì¡´ ê°€ì¤‘ì¹˜ë“¤ì„ ì•½ê°„ ì¡°ì •
            self.quality_criteria['page_reference']['weight'] = 0.20
            self.quality_criteria['specific_info']['weight'] = 0.25
            self.quality_criteria['answer_length']['weight'] = 0.15
            self.quality_criteria['safety_info']['weight'] = 0.15
            self.quality_criteria['error_free']['weight'] = 0.10
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_score = sum(
            scores[criterion] * self.quality_criteria[criterion]['weight']
            for criterion in scores
        )
        
        # ì‹ ë¢°ë„ ë“±ê¸‰ ê²°ì •
        reliability_grade = self._get_reliability_grade(total_score)
        
        return {
            'total_score': round(total_score, 2),
            'max_score': 1.0,
            'percentage': round(total_score * 100, 1),
            'reliability_grade': reliability_grade,
            'detailed_scores': scores,
            'evaluation_summary': self._generate_summary(scores, total_score)
        }
    
    def _evaluate_page_reference(self, answer: str) -> float:
        """í˜ì´ì§€ ì°¸ì¡° ì •ë³´ í‰ê°€"""
        if 'ğŸ“š' in answer and 'í˜ì´ì§€' in answer:
            # êµ¬ì²´ì  í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸
            page_pattern = r'í˜ì´ì§€[:\s]*(\d+(?:-\d+)?(?:,\s*\d+(?:-\d+)?)*)'
            if re.search(page_pattern, answer):
                return 1.0
            else:
                return 0.5  # í˜ì´ì§€ ì–¸ê¸‰ì€ ìˆì§€ë§Œ êµ¬ì²´ì  ë²ˆí˜¸ ì—†ìŒ
        return 0.0
    
    def _evaluate_specific_info(self, answer: str) -> float:
        """êµ¬ì²´ì  ì •ë³´ í¬í•¨ ì—¬ë¶€ í‰ê°€"""
        score = 0.0
        answer_lower = answer.lower()
        
        # êµ¬ì²´ì  í‚¤ì›Œë“œ í™•ì¸
        keyword_count = sum(1 for keyword in self.specific_keywords if keyword in answer_lower)
        if keyword_count >= 3:
            score += 0.6
        elif keyword_count >= 1:
            score += 0.3
        
        # ìˆ«ì ì •ë³´ í™•ì¸
        number_pattern = r'\d+(?:\.\d+)?(?:\s*[a-zA-Z%]+)?'
        numbers = re.findall(number_pattern, answer)
        if len(numbers) >= 2:
            score += 0.4
        elif len(numbers) >= 1:
            score += 0.2
        
        return min(score, 1.0)
    
    def _evaluate_answer_length(self, answer: str) -> float:
        """ë‹µë³€ ê¸¸ì´ ì ì ˆì„± í‰ê°€"""
        length = len(answer)
        if 200 <= length <= 800:
            return 1.0
        elif 100 <= length < 200 or 800 < length <= 1200:
            return 0.7
        elif 50 <= length < 100 or 1200 < length <= 1500:
            return 0.4
        else:
            return 0.1
    
    def _evaluate_safety_info(self, answer: str) -> float:
        """ì•ˆì „ ê´€ë ¨ ì •ë³´ í‰ê°€"""
        answer_lower = answer.lower()
        safety_count = sum(1 for keyword in self.safety_keywords if keyword in answer_lower)
        
        if safety_count >= 3:
            return 1.0
        elif safety_count >= 2:
            return 0.7
        elif safety_count >= 1:
            return 0.4
        else:
            return 0.0
    
    def _evaluate_error_free(self, answer: str) -> float:
        """ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ í‰ê°€"""
        error_indicators = ['ì˜¤ë¥˜', 'ì‹¤íŒ¨', 'ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤', 'error', 'ì—†ìŠµë‹ˆë‹¤']
        answer_lower = answer.lower()
        
        for indicator in error_indicators:
            if indicator in answer_lower:
                return 0.0
        return 1.0
    
    def _evaluate_search_quality(self, search_results: List[Dict]) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€"""
        if not search_results:
            return 0.0
        
        score = 0.0
        
        # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ í‰ê°€ (ì ì ˆí•œ ê°œìˆ˜)
        result_count = len(search_results)
        if 3 <= result_count <= 5:
            score += 0.3
        elif 1 <= result_count <= 7:
            score += 0.2
        
        # í˜ì´ì§€ ì •ë³´ í¬í•¨ ë¹„ìœ¨
        page_info_count = sum(1 for result in search_results if result.get('page', 0) > 0)
        if page_info_count > 0:
            page_ratio = page_info_count / result_count
            score += 0.3 * page_ratio
        
        # ê²€ìƒ‰ ì ìˆ˜ í‰ê°€ (ìˆëŠ” ê²½ìš°)
        scores = [result.get('score', 0) for result in search_results if result.get('score', 0) > 0]
        if scores:
            avg_score = sum(scores) / len(scores)
            if avg_score > 0.7:
                score += 0.2
            elif avg_score > 0.5:
                score += 0.1
        
        # ë‚´ìš© ê¸¸ì´ í‰ê°€ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ì§€)
        content_lengths = [len(result.get('content', '')) for result in search_results]
        if content_lengths:
            avg_length = sum(content_lengths) / len(content_lengths)
            if 100 <= avg_length <= 500:
                score += 0.2
            elif 50 <= avg_length <= 800:
                score += 0.1
        
        return min(score, 1.0)
    
    def _get_reliability_grade(self, score: float) -> str:
        """ì‹ ë¢°ë„ ë“±ê¸‰ ê²°ì •"""
        if score >= 0.9:
            return "ë§¤ìš° ë†’ìŒ (A+)"
        elif score >= 0.8:
            return "ë†’ìŒ (A)"
        elif score >= 0.7:
            return "ì–‘í˜¸ (B)"
        elif score >= 0.6:
            return "ë³´í†µ (C)"
        elif score >= 0.5:
            return "ë‚®ìŒ (D)"
        else:
            return "ë§¤ìš° ë‚®ìŒ (F)"
    
    def _generate_summary(self, scores: Dict[str, float], total_score: float) -> str:
        """í‰ê°€ ìš”ì•½ ìƒì„±"""
        strong_points = []
        weak_points = []
        
        for criterion, score in scores.items():
            description = self.quality_criteria[criterion]['description']
            if score >= 0.8:
                strong_points.append(description)
            elif score <= 0.3:
                weak_points.append(description)
        
        summary = f"ì „ì²´ ì ìˆ˜: {total_score:.2f}/1.0 ({total_score*100:.1f}%)\n"
        
        if strong_points:
            summary += f"âœ… ê°•ì : {', '.join(strong_points)}\n"
        
        if weak_points:
            summary += f"âš ï¸ ê°œì„ ì : {', '.join(weak_points)}"
        
        return summary
    
    def batch_evaluate(self, qa_pairs: List[Dict]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ë‹µë³€ ì¼ê´„ í‰ê°€"""
        evaluations = []
        
        for qa_pair in qa_pairs:
            question = qa_pair.get('question', '')
            answer = qa_pair.get('answer', '')
            search_results = qa_pair.get('search_results', [])
            
            evaluation = self.evaluate_answer(question, answer, search_results)
            evaluation['question'] = question
            evaluations.append(evaluation)
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_scores = [eval['total_score'] for eval in evaluations]
        avg_score = sum(total_scores) / len(total_scores) if total_scores else 0
        
        grade_distribution = {}
        for evaluation in evaluations:
            grade = evaluation['reliability_grade']
            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
        
        return {
            'evaluations': evaluations,
            'summary': {
                'total_questions': len(evaluations),
                'average_score': round(avg_score, 2),
                'average_percentage': round(avg_score * 100, 1),
                'grade_distribution': grade_distribution,
                'high_quality_count': len([e for e in evaluations if e['total_score'] >= 0.8])
            }
        }

"""
ë¬¸ì„œ ë¡œë”© ìœ í‹¸ë¦¬í‹°
"""

import os
from typing import List
from pathlib import Path
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

from ..config.settings import CHUNK_SIZE, CHUNK_OVERLAP


class DocumentLoader:
    """PDF ë¬¸ì„œ ë¡œë”© ë° ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    
    def __init__(self, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
    
    def load_pdf(self, pdf_path: str) -> List[Document]:
        """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¡œ ë³€í™˜"""
        try:
            # íŒŒì¼ ì¡´ìž¬ í™•ì¸
            if not os.path.exists(pdf_path):
                raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
            
            print(f"ðŸ“„ PDF íŒŒì¼ ë¡œë”© ì¤‘: {Path(pdf_path).name}")
            
            # PDF ë¡œë”ë¡œ ë¬¸ì„œ ë¡œë“œ
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            
            print(f"   ðŸ“– ë¡œë“œëœ íŽ˜ì´ì§€ ìˆ˜: {len(documents)}")
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            for i, doc in enumerate(documents):
                doc.metadata.update({
                    'source': pdf_path,
                    'page': i + 1,
                    'total_pages': len(documents)
                })
            
            return documents
            
        except Exception as e:
            print(f"âŒ PDF ë¡œë”© ì˜¤ë¥˜: {str(e)}")
            return []
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• """
        try:
            print(f"âœ‚ï¸  ë¬¸ì„œ ë¶„í•  ì¤‘ (ì²­í¬ í¬ê¸°: {self.chunk_size}, ê²¹ì¹¨: {self.chunk_overlap})")
            
            split_docs = self.text_splitter.split_documents(documents)
            
            print(f"   ðŸ“‘ ë¶„í• ëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(split_docs)}")
            
            # ë¶„í• ëœ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ë³´ê°•
            for i, doc in enumerate(split_docs):
                doc.metadata.update({
                    'chunk_id': i,
                    'chunk_size': len(doc.page_content)
                })
            
            return split_docs
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ë¶„í•  ì˜¤ë¥˜: {str(e)}")
            return []
    
    def load_and_split_pdf(self, pdf_path: str) -> List[Document]:
        """PDF ë¡œë“œ ë° ë¶„í• ì„ í•œë²ˆì— ìˆ˜í–‰"""
        documents = self.load_pdf(pdf_path)
        if not documents:
            return []
        
        return self.split_documents(documents)
    
    def get_document_stats(self, documents: List[Document]) -> dict:
        """ë¬¸ì„œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not documents:
            return {"total_docs": 0, "total_chars": 0, "avg_chunk_size": 0}
        
        total_chars = sum(len(doc.page_content) for doc in documents)
        avg_chunk_size = total_chars / len(documents)
        
        pages = set()
        for doc in documents:
            if 'page' in doc.metadata:
                pages.add(doc.metadata['page'])
        
        return {
            "total_docs": len(documents),
            "total_chars": total_chars,
            "avg_chunk_size": round(avg_chunk_size, 2),
            "unique_pages": len(pages),
            "min_chunk_size": min(len(doc.page_content) for doc in documents),
            "max_chunk_size": max(len(doc.page_content) for doc in documents)
        }

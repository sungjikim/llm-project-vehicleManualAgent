"""
ì½œë°± í•¸ë“¤ëŸ¬ - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì‹¤ì‹œê°„ ì•Œë¦¼
"""

import time
import json
from typing import Any, Dict, List, Optional, Union
from datetime import datetime
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_core.messages import BaseMessage
from langchain_core.documents import Document


class PerformanceMonitoringHandler(BaseCallbackHandler):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì½œë°± í•¸ë“¤ëŸ¬"""
    
    def __init__(self, enable_detailed_logging: bool = True):
        super().__init__()
        self.enable_detailed_logging = enable_detailed_logging
        self.session_start_time = None
        self.query_start_time = None
        self.llm_calls = []
        self.retriever_calls = []
        self.total_tokens = 0
        self.total_cost = 0.0
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            "total_queries": 0,
            "total_llm_calls": 0,
            "total_retriever_calls": 0,
            "total_tokens_used": 0,
            "total_cost": 0.0,
            "average_response_time": 0.0,
            "session_duration": 0.0
        }
    
    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs) -> None:
        """ì²´ì¸ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        if self.session_start_time is None:
            self.session_start_time = time.time()
        
        self.query_start_time = time.time()
        self.performance_stats["total_queries"] += 1
        
        if self.enable_detailed_logging:
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘")
    
    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        """ì²´ì¸ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        if self.query_start_time:
            query_duration = time.time() - self.query_start_time
            
            # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
            total_queries = self.performance_stats["total_queries"]
            current_avg = self.performance_stats["average_response_time"]
            self.performance_stats["average_response_time"] = (
                (current_avg * (total_queries - 1) + query_duration) / total_queries
            )
            
            if self.enable_detailed_logging:
                print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ ({query_duration:.2f}ì´ˆ)")
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        """LLM í˜¸ì¶œ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        self.performance_stats["total_llm_calls"] += 1
        # serializedê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        if serialized and serialized.get("id"):
            model_name = serialized["id"][-1] if serialized["id"] else "unknown"
        else:
            model_name = "unknown"
            
        llm_call = {
            "start_time": time.time(),
            "prompts": prompts if self.enable_detailed_logging else len(prompts),
            "model": model_name
        }
        self.llm_calls.append(llm_call)
        
        if self.enable_detailed_logging:
            print(f"ğŸ¤– [{datetime.now().strftime('%H:%M:%S')}] LLM í˜¸ì¶œ ì‹œì‘ (ëª¨ë¸: {llm_call['model']})")
    
    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        """LLM í˜¸ì¶œ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        if self.llm_calls:
            current_call = self.llm_calls[-1]
            current_call["end_time"] = time.time()
            current_call["duration"] = current_call["end_time"] - current_call["start_time"]
            
            # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
            if hasattr(response, 'llm_output') and response.llm_output:
                token_usage = response.llm_output.get('token_usage', {})
                tokens_used = token_usage.get('total_tokens', 0)
                self.total_tokens += tokens_used
                self.performance_stats["total_tokens_used"] += tokens_used
                
                # ë¹„ìš© ì¶”ì • (GPT-4o-mini ê¸°ì¤€: $0.15/1M input tokens, $0.6/1M output tokens)
                input_tokens = token_usage.get('prompt_tokens', 0)
                output_tokens = token_usage.get('completion_tokens', 0)
                cost = (input_tokens * 0.15 / 1_000_000) + (output_tokens * 0.6 / 1_000_000)
                self.total_cost += cost
                self.performance_stats["total_cost"] += cost
                
                current_call["tokens"] = tokens_used
                current_call["cost"] = cost
                
                if self.enable_detailed_logging:
                    print(f"ğŸ’° [{datetime.now().strftime('%H:%M:%S')}] LLM ì™„ë£Œ "
                          f"({tokens_used} í† í°, ${cost:.4f}, {current_call['duration']:.2f}ì´ˆ)")
    
    def on_retriever_start(self, serialized: Dict[str, Any], query: str, **kwargs) -> None:
        """ë¦¬íŠ¸ë¦¬ë²„ í˜¸ì¶œ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        self.performance_stats["total_retriever_calls"] += 1
        
        # serializedê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        if serialized and serialized.get("id"):
            retriever_type = serialized["id"][-1] if serialized["id"] else "unknown"
        else:
            retriever_type = "unknown"
            
        retriever_call = {
            "start_time": time.time(),
            "query": query[:100] + "..." if len(query) > 100 else query,
            "type": retriever_type
        }
        self.retriever_calls.append(retriever_call)
        
        if self.enable_detailed_logging:
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] ê²€ìƒ‰ ì‹œì‘ (íƒ€ì…: {retriever_call['type']})")
    
    def on_retriever_end(self, documents: List[Document], **kwargs) -> None:
        """ë¦¬íŠ¸ë¦¬ë²„ í˜¸ì¶œ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        if self.retriever_calls:
            current_call = self.retriever_calls[-1]
            current_call["end_time"] = time.time()
            current_call["duration"] = current_call["end_time"] - current_call["start_time"]
            current_call["documents_found"] = len(documents)
            
            if self.enable_detailed_logging:
                print(f"ğŸ“„ [{datetime.now().strftime('%H:%M:%S')}] ê²€ìƒ‰ ì™„ë£Œ "
                      f"({len(documents)}ê°œ ë¬¸ì„œ, {current_call['duration']:.2f}ì´ˆ)")
    
    def on_chain_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs) -> None:
        """ì²´ì¸ ì˜¤ë¥˜ ì‹œ í˜¸ì¶œ"""
        print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] ì˜¤ë¥˜ ë°œìƒ: {str(error)}")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if self.session_start_time:
            self.performance_stats["session_duration"] = time.time() - self.session_start_time
        
        return {
            **self.performance_stats,
            "llm_calls_detail": self.llm_calls[-5:] if self.enable_detailed_logging else [],
            "retriever_calls_detail": self.retriever_calls[-5:] if self.enable_detailed_logging else []
        }
    
    def print_performance_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        stats = self.get_performance_summary()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸")
        print("=" * 60)
        print(f"ğŸ• ì„¸ì…˜ ì‹œê°„: {stats['session_duration']:.1f}ì´ˆ")
        print(f"â“ ì´ ì¿¼ë¦¬ ìˆ˜: {stats['total_queries']}ê°œ")
        print(f"ğŸ¤– LLM í˜¸ì¶œ: {stats['total_llm_calls']}íšŒ")
        print(f"ğŸ” ê²€ìƒ‰ í˜¸ì¶œ: {stats['total_retriever_calls']}íšŒ")
        print(f"ğŸ¯ í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['average_response_time']:.2f}ì´ˆ")
        print(f"ğŸ’ ì´ í† í° ì‚¬ìš©: {stats['total_tokens_used']:,}ê°œ")
        print(f"ğŸ’° ì´ ì˜ˆìƒ ë¹„ìš©: ${stats['total_cost']:.4f}")
        print("=" * 60)


class RealTimeNotificationHandler(BaseCallbackHandler):
    """ì‹¤ì‹œê°„ ì•Œë¦¼ ì½œë°± í•¸ë“¤ëŸ¬"""
    
    def __init__(self, enable_progress_bar: bool = True, enable_notifications: bool = True):
        super().__init__()
        self.enable_progress_bar = enable_progress_bar
        self.enable_notifications = enable_notifications
        self.current_step = 0
        self.total_steps = 0
        self.step_descriptions = []
        
    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs) -> None:
        """ì²´ì¸ ì‹œì‘ ì•Œë¦¼"""
        if self.enable_notifications:
            query = ""
            if isinstance(inputs, dict):
                query = inputs.get("query", inputs.get("input", ""))
            elif hasattr(inputs, 'content'):
                query = str(inputs.content)
            elif isinstance(inputs, str):
                query = inputs
            else:
                query = str(inputs)
            
            if len(query) > 50:
                query = query[:50] + "..."
            
            print(f"\nğŸš€ ì²˜ë¦¬ ì‹œì‘: {query}")
            
        # ì§„í–‰ ë‹¨ê³„ ì„¤ì •
        self.current_step = 0
        self.total_steps = 4  # ë¶„ì„ -> ê²€ìƒ‰ -> ì¬ìˆœìœ„í™” -> ë‹µë³€ìƒì„±
        self.step_descriptions = [
            "ì¿¼ë¦¬ ë¶„ì„ ì¤‘...",
            "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...",
            "ê²°ê³¼ ìµœì í™” ì¤‘...",
            "ë‹µë³€ ìƒì„± ì¤‘..."
        ]
        
        if self.enable_progress_bar:
            self._print_progress_bar()
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        """LLM ì‹œì‘ ì•Œë¦¼"""
        self.current_step += 1
        if self.enable_progress_bar and self.current_step <= len(self.step_descriptions):
            self._print_progress_bar()
    
    def on_retriever_start(self, serialized: Dict[str, Any], query: str, **kwargs) -> None:
        """ê²€ìƒ‰ ì‹œì‘ ì•Œë¦¼"""
        if self.enable_notifications:
            # serializedê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            if serialized and serialized.get("id"):
                retriever_type = serialized["id"][-1] if serialized["id"] else "unknown"
            else:
                retriever_type = "unknown"
            print(f"ğŸ” {retriever_type} ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
    
    def on_retriever_end(self, documents: List[Document], **kwargs) -> None:
        """ê²€ìƒ‰ ì™„ë£Œ ì•Œë¦¼"""
        if self.enable_notifications:
            print(f"ğŸ“„ {len(documents)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
    
    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        """ì²´ì¸ ì™„ë£Œ ì•Œë¦¼"""
        if self.enable_progress_bar:
            self.current_step = self.total_steps
            self._print_progress_bar()
            print()  # ì¤„ë°”ê¿ˆ
        
        if self.enable_notifications:
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
    
    def on_chain_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs) -> None:
        """ì˜¤ë¥˜ ì•Œë¦¼"""
        print(f"\nğŸš¨ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ!")
        print(f"ğŸ’¥ ì˜¤ë¥˜ ë‚´ìš©: {str(error)}")
        
        # ì¼ë°˜ì ì¸ í•´ê²° ë°©ë²• ì œì•ˆ
        if "API" in str(error):
            print("ğŸ’¡ í•´ê²° ë°©ë²•: OpenAI API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        elif "token" in str(error).lower():
            print("ğŸ’¡ í•´ê²° ë°©ë²•: ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ì–´ë³´ì„¸ìš”.")
        elif "network" in str(error).lower() or "connection" in str(error).lower():
            print("ğŸ’¡ í•´ê²° ë°©ë²•: ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    def _print_progress_bar(self):
        """ì§„í–‰ë¥  ë°” ì¶œë ¥"""
        if not self.enable_progress_bar:
            return
            
        progress = min(self.current_step / self.total_steps, 1.0)
        bar_length = 30
        filled_length = int(bar_length * progress)
        
        bar = "â–ˆ" * filled_length + "â–‘" * (bar_length - filled_length)
        percentage = int(progress * 100)
        
        current_desc = ""
        if self.current_step <= len(self.step_descriptions):
            current_desc = self.step_descriptions[self.current_step - 1] if self.current_step > 0 else self.step_descriptions[0]
        
        print(f"\rğŸ”„ [{bar}] {percentage}% - {current_desc}", end="", flush=True)


class AlertHandler(BaseCallbackHandler):
    """ê²½ê³  ë° ì•Œë¦¼ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, token_limit: int = 100000, cost_limit: float = 1.0):
        super().__init__()
        self.token_limit = token_limit
        self.cost_limit = cost_limit
        self.session_tokens = 0
        self.session_cost = 0.0
    
    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        """LLM ì™„ë£Œ í›„ ì‚¬ìš©ëŸ‰ ì²´í¬"""
        if hasattr(response, 'llm_output') and response.llm_output:
            token_usage = response.llm_output.get('token_usage', {})
            tokens_used = token_usage.get('total_tokens', 0)
            self.session_tokens += tokens_used
            
            # ë¹„ìš© ê³„ì‚°
            input_tokens = token_usage.get('prompt_tokens', 0)
            output_tokens = token_usage.get('completion_tokens', 0)
            cost = (input_tokens * 0.15 / 1_000_000) + (output_tokens * 0.6 / 1_000_000)
            self.session_cost += cost
            
            # í† í° í•œë„ ì²´í¬
            if self.session_tokens > self.token_limit * 0.8:  # 80% ë„ë‹¬ ì‹œ ê²½ê³ 
                print(f"\nâš ï¸ í† í° ì‚¬ìš©ëŸ‰ ê²½ê³ : {self.session_tokens:,}/{self.token_limit:,} "
                      f"({self.session_tokens/self.token_limit*100:.1f}%)")
            
            # ë¹„ìš© í•œë„ ì²´í¬
            if self.session_cost > self.cost_limit * 0.8:  # 80% ë„ë‹¬ ì‹œ ê²½ê³ 
                print(f"\nğŸ’¸ ë¹„ìš© ê²½ê³ : ${self.session_cost:.4f}/${self.cost_limit:.2f} "
                      f"({self.session_cost/self.cost_limit*100:.1f}%)")
    
    def get_usage_summary(self) -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ ìš”ì•½ ë°˜í™˜"""
        return {
            "tokens_used": self.session_tokens,
            "token_limit": self.token_limit,
            "token_usage_percentage": (self.session_tokens / self.token_limit) * 100,
            "cost_incurred": self.session_cost,
            "cost_limit": self.cost_limit,
            "cost_usage_percentage": (self.session_cost / self.cost_limit) * 100
        }
